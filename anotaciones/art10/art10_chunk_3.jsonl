{"id":3,"text":"##ation Date : 10 September 2019 Original Paper Downloaded from https : / / academic. oup. com / bioinformatics / article / 36 / 4 / 1234 / 5566506 by guest on 30 October 2025 However, directly applying state-of-the-art NLP methodologies to biomedical text mining has limitations. First, as recent word rep-resentation models such as Word2Vec ( Mikolov, 2013 ), ELMo ( Peters, 2018 ) and BERT ( Devlin, 2019 ) are trained and tested mainly on datasets containing general domain texts ( e. g. Wikipedia ), it is difficult to estimate their performance on datasets containing biomedical texts. Also, the word distributions of general and biomedical corpora are quite different, which can often be a problem for biomedical text mining models. As a result, recent mod-els in biomedical text mining rely largely on adapted versions of word representations ( Habibi, 2017 ; Pyysalo, 2013 ). In this study, we hypothesize that current state-of-the-art word representation models such as BERT need to be trained on biomed-ical corpora to be effective in biomedical text mining tasks. Previously, Word2Vec, which is one of the most widely known con-text independent word representation models, was trained on bio-medical corpora which contain terms and expressions that are usually not included in a general domain corpus ( Pyysalo, 2013 ). While ELMo and BERT have proven the effectiveness of con-textualized word representations, they cannot obtain high perform-ance on biomedical corpora because they are pre-trained on only general domain corpora. As BERT achieves very strong results on various NLP tasks while using almost the same structure across the tasks, adapting BERT for the biomedical domain could potentially benefit numerous biomedical NLP researches. 2 Approach In this article, we introduce BioBERT, which is a pre-trained language representation model for the biomedical domain. The overall process of pre-training and fine-tuning BioBERT is illustrated in First, we initialize BioBERT with weights from BERT, which was pre-trained on general domain corpora ( English Wikipedia and BooksCorpus ). Then, BioBERT is","label":[[388,392,"MODEL"],[980,984,"MODEL"],[1347,1351,"MODEL"],[1546,1550,"MODEL"],[1666,1670,"MODEL"],[2014,2018,"MODEL"],[1800,1807,"MODEL"],[1941,1948,"MODEL"],[1988,1995,"MODEL"],[2113,2120,"MODEL"],[334,342,"MODEL"],[1088,1096,"MODEL"],[1929,1940,"TECHNIQUE"]],"Comments":[]}
