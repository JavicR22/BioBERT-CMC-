{"id":4,"text":"pre-trained on biomedical domain corpora ( PubMed abstracts and PMC full-text articles ). To show the ef-fectiveness of our approach in biomedical text mining, BioBERT is fine-tuned and evaluated on three popular biomedical text mining tasks ( NER, RE and QA ). We test various pre-training strategies with differ-ent combinations and sizes of general domain corpora and biomedical corpora, and analyze the effect of each corpus on pre-training. We also provide in-depth analyses of BERT and BioBERT to show the necessity of our pre-training strategies. The contributions of our paper are as follows : BioBERT is the first domain-specific BERT based model pre-trained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs. We show that pre-training BERT on biomedical corpora largely improves its performance. BioBERT obtains higher F1 scores in biomedical NER ( 0. 62 ) and biomedical RE ( 2. 80 ), and a higher MRR score ( 12. 24 ) in biomedical QA than the current state-of-the-art models. Compared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA, our model BioBERT achieves state-of-the-art performance on various biomedical text mining tasks, while requiring only minimal architectural modifications. We make our pre-processed datasets, the pre-trained weights of BioBERT and the source code for fine-tuning BioBERT publicly available. 3 Materials and methods BioBERT basically has the same structure as BERT. We briefly dis-cuss the recently proposed BERT, and then we describe in detail the pre-training and fine-tuning process of BioBERT. 3. 1 BERT : bidirectional encoder representations from transformers Learning word representations from a large amount of unannotated text is a long-established method. While previous models ( e. g. Word2Vec ( Mikolov, 2013 ), GloVe ( Pennington, 2014 ) ) focused on learning context independent word representations, re-cent works have focused on learning context dependent word","label":[[1668,1680,"MODEL"],[483,487,"MODEL"],[639,643,"MODEL"],[755,759,"MODEL"],[1475,1479,"MODEL"],[1523,1527,"MODEL"],[1618,1622,"MODEL"],[160,167,"MODEL"],[492,499,"MODEL"],[602,609,"MODEL"],[816,823,"MODEL"],[1127,1134,"MODEL"],[1335,1342,"MODEL"],[1379,1386,"MODEL"],[1431,1438,"MODEL"],[1604,1611,"MODEL"],[1811,1819,"MODEL"],[1839,1844,"MODEL"],[244,247,"TECHNIQUE"],[863,866,"TECHNIQUE"],[1106,1109,"TECHNIQUE"],[171,181,"TECHNIQUE"],[1367,1378,"TECHNIQUE"],[1581,1592,"TECHNIQUE"],[1681,1689,"TECHNIQUE"],[919,922,"METRIC"],[43,49,"DATASET"]],"Comments":[]}
