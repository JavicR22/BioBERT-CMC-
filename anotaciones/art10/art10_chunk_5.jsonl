{"id":5,"text":"repre-sentations. For instance, ELMo ( Peters, 2018 ) uses a bidirectional language model, while CoVe ( McCann, 2017 ) uses machine translation to embed context information into word representations. BERT ( Devlin, 2019 ) is a contextualized word representa-tion model that is based on a masked language model and pre-trained using bidirectional transformers ( Vaswani, 2017 ). Due to the nature of language modeling where future words cannot be seen, previous language models were limited to a combination of two unidirectional language models ( i. e. left-to-right and right-to-left ). BERT uses a masked language model that predicts randomly masked words in a sequence, and hence can be used for learning bi-directional representations. Also, it obtains state-of-the-art perform-ance on most NLP tasks, while requiring minimal task-specific architectural modification. According to the authors of BERT, incorporating information from bidirectional representations, rather than unidirectional representations, is crucial for representing words in natural language. We hypothesize that such bidirectional repre-sentations are also critical in biomedical text mining as complex relationships between biomedical terms often exist in a biomedical corpus ( Krallinger, 2017 ). Due to the space limitations, we refer readers to Devlin for a more detailed description of BERT. 3. 2 Pre-training BioBERT As a general purpose language representation model, BERT was pre-trained on English Wikipedia and BooksCorpus. However, biomed-ical domain texts contain a considerable number of domain-specific Overview of the pre-training and fine-tuning of BioBERT BioBERT 1235 Downloaded from https : / / academic. oup. com / bioinformatics / article / 36 / 4 / 1234 / 5566506 by guest on 30 October 2025 proper nouns ( e. g. BRCA1, c. 248T > C ) and terms ( e. g. transcrip-tional, antimicrobial ), which are understood mostly by biomedical researchers. As a result, NLP models designed for general purpose language understanding often obtains poor performance in biomed-ical text mining tasks. In this work, we pre-train BioBERT","label":[[346,358,"MODEL"],[200,204,"MODEL"],[588,592,"MODEL"],[900,904,"MODEL"],[1366,1370,"MODEL"],[1450,1454,"MODEL"],[1390,1397,"MODEL"],[1640,1647,"MODEL"],[1648,1655,"MODEL"],[2107,2114,"MODEL"],[1170,1177,"MODEL"],[1625,1636,"TECHNIQUE"],[699,707,"TECHNIQUE"],[124,143,"APPLICATION"]],"Comments":[]}
