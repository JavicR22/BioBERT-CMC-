{"id":7,"text":"has a simple architecture based on bidirectional trans-formers. BERT uses a single output layer based on the representa-tions from its last layer to compute only token level BIO2 probabilities. Note that while previous works in biomedical NER often used word embeddings trained on PubMed or PMC corpora ( Habibi, 2017 ; Yoon, 2019 ), BioBERT directly learns WordPiece embeddings during pre-training and fine-tuning. For the evaluation metrics of NER, we used entity level precision, recall and F1 score. Relation extraction is a task of classifying relations of named entities in a biomedical corpus. We utilized the sentence classifier of the original version of BERT, which uses a token for the clas-sification of relations. Sentence classification is performed using a single output layer based on a token representation from BERT. We anonymized target named entities in a sentence using pre-defined tags such as @ GENE. For instance, a sentence with two target entities ( gene and disease in this case ) is represented as \" Serine at position 986 of @ GENE. \" The preci-sion, recall and F1 scores on the RE task are reported. Question answering is a task of answering questions posed in natural language given related passages. To fine-tune BioBERT for QA, we used the same BERT architecture used for SQuAD ( Rajpurkar, 2016 ). We used the BioASQ factoid datasets be-cause their format is similar to that of SQuAD. Token level proba-bilities for the start / end location of answer phrases are computed using a single output layer. However, we observed that about 30 % of the BioASQ factoid questions were unanswerable in an extractive QA setting as the exact answers did not appear in the given pas-sages. Like Wiese, we excluded the samples with un-answerable questions from the training sets. Also, we used the same pre-training process of Wiese, which uses SQuAD, and it largely improved the performance of both BERT and BioBERT. We used the following evaluation metrics from BioASQ : strict accur-acy, lenient accuracy and mean","label":[[49,62,"MODEL"],[334,341,"MODEL"],[1245,1252,"MODEL"],[1928,1935,"MODEL"],[64,68,"MODEL"],[664,668,"MODEL"],[829,833,"MODEL"],[1278,1282,"MODEL"],[1919,1923,"MODEL"],[403,414,"TECHNIQUE"],[494,502,"TECHNIQUE"],[239,242,"TECHNIQUE"],[446,449,"TECHNIQUE"],[472,481,"METRIC"],[1068,1078,"METRIC"],[2018,2026,"METRIC"],[483,489,"METRIC"],[1080,1086,"METRIC"],[1344,1350,"DATASET"],[1579,1585,"DATASET"],[1983,1989,"DATASET"],[281,287,"DATASET"],[1305,1310,"DATASET"],[1412,1417,"DATASET"],[1864,1869,"DATASET"]],"Comments":[]}
