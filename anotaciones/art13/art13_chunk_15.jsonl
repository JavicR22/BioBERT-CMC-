{"id":15,"text":"169 BERTS 0. 654 0. 654 0. 667 0. 445 0. 654 0. 654 0. 601 0. 678 MIMIC-CXR R-L 0. 051 0. 172 0. 360 0. 400 0. 059 0. 137 0. 367 0. 418 BERTS 0. 566 0. 697 0. 791 0. 797 0. 577 0. 649 0. 784 0. 787 BioNLI Macro-F1 0. 109 0. 060 0. 185 0. 195 0. 285 0. 499 0. 345 0. 436 MedNLI Macro-F1 0. 172 0. 206 0. 457 0. 472 0. 265 0. 256 0. 657 0. 675 https : / / doi. org / 10. 1038 / s41746-025-01533-1 Article npj Digital Medicine | 8 : 141 5 may impact Me LLaMA ’ s generalizability on real-world scenarios. This limitation stems primarily from privacy concerns surrounding clinical data, which significantly restrict the availability of large-scale EHR datasets. In this study, we included only MIMIC in the trainingof Me LLaMA because it is readily available for public dissemination through the established MIMIC data access procedures. Moving forward, we plan to train Me LLaMA on much larger proprietary clinical datasets, such as EHRs from Yale New Haven Health and the University of Florida Health. However, the terms of distribution and dissemination for models trained on such proprietary data will need to be carefully negotiated with our institutions ’ data governance committees to ensure the safety and confidentiality of clinical data. Methods We utilized LLaMA26 as the backbone model and developed Me-LLaMA through the process of continual pre-training and instruction tuning of LLaMA2, using 129B tokens and 214 K instruction tuning samples from general, biomedical, andclinicaldomains. study. presents the comparison of Me-LLaMA models and existing open source medical LLMs. Continual pre-training data To effectively adapt backbone LLaMA2 models for the medical domain through continual pre-training, we developed a mixed continual pre-training dataset, comprised","label":[[1264,1270,"MODEL"],[450,455,"MODEL"],[717,722,"MODEL"],[870,875,"MODEL"],[1311,1316,"MODEL"],[1535,1540,"MODEL"],[66,75,"DATASET"]],"Comments":[]}
