{"id":10,"text":"applied together, as seen in Me-LLaMA-70B-chat, which outperforms all other configurations. This indicates that while instruction tuning is the most efficient approach for improving task performance, continual pre-training provides a complementary boost, particularly for larger models where additional domain adaptation enhances overall effectiveness. Discussion We introduced a novel medical LLM family including, Me-LLaMA 13B and Me-LLaMA 70B, which encode comprehensive medical knowledge, along with their chat-optimized variants : Me-LLaMA-13 / 70B-chat, with strong zero-shot learning ability, for medical applications. These models were developed through the continual pre-training and instruction tuning of LLaMA2models, usingthe largestand mostcomprehensivebiomedicaland clinical data. Compared to existing studies, we perform the most compre-hensive evaluation, covering six critical text analysis tasks. Our evaluations reveal that Me-LLaMA models outperform existing open-source medical LLMs in various learning scenarios, showing less susceptibility to cata-strophic forgetting and achieving competitive results against major com-mercialmodelsincludingChatGPTandGPT-4. Ourworkpavesthewayfor more accurate, reliable, and comprehensive medical LLMs, and underscores the potential of LLMs on medical applications. Despitethesestrengths, weobservedcertainchallengesinspecifictasks, such as NER and RE14, 15, where even advanced models like GPT-4 exhibited low performance. When compared with other NLP tasks with higher per-formance, we noticed that one of the main reasons for low performance is that LLMs ’ responses often lacked the conciseness and precision expected, with instances of missing outputs noted. The unexpected outputs also cause significant challenges to automatic evaluation metrics. Therefore, more investigation is needed to further improve medical LLMs ’ performance across tasks in the zero-shot setting and enhance the automatic assessment of these medical LLMs ’ zero-shot capabilities. For the complex clinical case diagnosis, the Me-LLaMA-chat model had competitive performance and even outperformed GPT-4 in human evaluation. Existing studies have demonstrated GPT-4 is arguably one of","label":[[1449,1452,"MODEL"],[2136,2139,"MODEL"],[2198,2201,"MODEL"],[32,37,"MODEL"],[419,424,"MODEL"],[436,441,"MODEL"],[539,544,"MODEL"],[946,951,"MODEL"],[2069,2074,"MODEL"],[2029,2036,"MODEL"],[572,590,"TECHNIQUE"],[1399,1402,"TECHNIQUE"],[118,136,"TECHNIQUE"],[693,711,"TECHNIQUE"],[582,590,"TECHNIQUE"],[1661,1670,"METRIC"]],"Comments":[]}
