{"id":18,"text":"the LLaMA2 13B and 70B models. These base models were then instruction-tuned to create the Me-LLaMA-13B-chat and Me-LLaMA-70B-chat models. The first phase aims to develop Me-LLaMA base models, and adapt LLaMA2 models to better understand and generate text relevant to the medical context using the pre-training datasets we constructed. The objective is to enhance the model ’ s ability to understand and generate domain-specific text by optimizing it to predict the next word in a sequence based on the preceding context. This training was executed on the University of Florida ’ s HiPerGator AI supercomputer with 160 A100 80GB GPUs. We employed the AdamW optimizer with hyperparameters set to β1 to 0. 9 and β2 to 0. 95, alongside a weight decay of 0. 00001 and a learning rate of 8e-6. We used a cosine learning rate scheduler with a 0. 05 warmup ratio for gradual adaptation to training complexity and bf16 precision for com-putational efficiency. Gradient accumulation was set to 16 steps, and training was limited to one epoch. We utilized DeepSpeed24 for model parallelism. We further fine-tuned Me-LLaMA base models to develop Me-LLaMA chat models, using the developed 214k instruction samples. In this phase, the models are trained to produce accurate and contextually appropriate responses to specific input instructions. Executed using 8 A100 GPUs, the fine-tuning process was set to run for 3 epochs with a learning rate of 1e-5. We used a weight decay of 0. 00001 and a warmup ratio of 0. 01 for regularization and gradual learning rate increase. We utilized LoRA-based25 parameter-efficient fine-tuning. Evaluation benchmark Existing studies2, 3, 9 in the medical domain have primarily focused on eval-uating the QA task. In this study, we build an extensive medical evaluation benchmark ( MIBE ), encompassing six critical text analysis tasks : QA, NER, RE, Text Classification, Text Summarization and NLI. These tasks collec-tively","label":[[94,99,"MODEL"],[116,121,"MODEL"],[174,179,"MODEL"],[1106,1111,"MODEL"],[1138,1143,"MODEL"],[4,10,"MODEL"],[1864,1867,"TECHNIQUE"],[1873,1892,"TECHNIQUE"],[1092,1102,"TECHNIQUE"],[1364,1375,"TECHNIQUE"],[1605,1616,"TECHNIQUE"],[1585,1616,"TECHNIQUE"],[1572,1576,"TECHNIQUE"],[1062,1079,"TECHNIQUE"],[1509,1523,"TECHNIQUE"],[735,747,"TECHNIQUE"],[1452,1464,"TECHNIQUE"],[651,656,"TECHNIQUE"],[843,849,"TECHNIQUE"],[1483,1489,"TECHNIQUE"],[766,774,"TECHNIQUE"],[1629,1638,"TECHNIQUE"],[1792,1801,"TECHNIQUE"],[911,920,"METRIC"],[1899,1912,"APPLICATION"],[466,470,"ARCHITECTURE"],[1046,1057,"TECHNOLOGY"]],"Comments":[]}
