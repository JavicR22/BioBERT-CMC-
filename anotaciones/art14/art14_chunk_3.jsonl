{"id":3,"text":"Biomedical Informatics, College of Medicine, University of Florida, Gainesville, FL, USA. 3School of Biomedical Informatics, University of Texas Health Science, Center at Houston, Houston, TX, USA. 4Department of Medicine ( Digestive Diseases ), Yale School of Medicine, Yale University, New Haven, CT, USA. 5These authors contributed equally : Qianqian Xie, Qingyu Chen, Aokun Chen. e-mail : hua. xu @ yale. edu ; bianji @ iu. edu npj Digital Medicine | 8 : 141 1 1234567890 ( ) :, ; 1234567890 ( ) :, ; clinical notes, we generated the largest biomedical pre-training dataset ( 129B tokens ), compared to the previous efforts ( i. e., 79B tokens in PMC-LLaMA as the highest ). Evaluations have predominantly centered on medical question-answering ( QA ) tasks, lacking comprehensive assess-ments on the generalizability of those foundation models across diverse medical tasks. To overcome these limitations, we present Me-LLaMA, a novel family of open-source medical large language models that uniquely integrate extensive domain-specific knowledge with robust instruction-following capabilities. Me-LLaMA comprises foundation models ( Me-LLaMA 13B and 70B ) and their chat-enhanced versions, developed through compre-hensive continual pretraining and instruction tuning of LLaMA26 models. Leveraging an extensive medical dataset-combining 129 billion pretrain-ingtokensand214, 000instructionsamplesfromscientificliterature, clinical guidelines, and electronic health record clinical notes-Me-LLaMA excels across a wide spectrum of medical text analysis and real-world clinical tasks. Prior studies2, 3, 7 â€“ 12 have primarily focused on evaluating the QA task. For example, PMC-LLaMA2 and Meditron9 evaluated their model performance on medical QA tasks derived from domain-specific literature, while MedAlpaca3 and ChatDoctor10 focused on conversational QA. In contrast, we conduct a comprehensive evaluation covering six critical tasks-question answering, relation extraction, named entity","label":[[1802,1812,"MODEL"],[655,660,"MODEL"],[924,929,"MODEL"],[1102,1107,"MODEL"],[1141,1146,"MODEL"],[1495,1500,"MODEL"],[1276,1282,"MODEL"],[1959,1978,"TECHNIQUE"],[1254,1272,"TECHNIQUE"],[1939,1957,"APPLICATION"]],"Comments":[]}
