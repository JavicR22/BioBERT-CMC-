{"id":4,"text":"recognition, text classification, text summarization, and natural language inference-across twelve datasets from both biomedical and clinical domains. Our results demonstrate that Me-LLaMA not only surpasses existing open-source medical LLMs in both zero-shot and supervised settings but also, with task-specific instruction tuning, outperforms leading commercial LLMs such as ChatGPT on seven out of eight datasets and GPT-4 on five out of eight datasets. Furthermore, to evaluate Me-LLaMA ’ s potential clinical utility, we assessed the models on complex clinical case diagnosis tasks, comparing their performance with other commercial LLMs using both automatic and human evaluations. Ourfindingsindicatethat Me-LLaMA ’ s performance is comparabletothatofChatGPTandGPT-4, despitetheirsubstantiallylarger model sizes. Our findings underscore the importance of combining domain-specific continual pretraining with instruction tuning to develop effective large language models for the medical domain. Recognizing the significant resources required, we have publicly released our Me-LLaMA models on PhysioNet under appropriate Data Use Agreements ( DUAs ) to lower bar-riersandfosterinnovationwithinthemedicalAIcommunity. Alongsidethe models, we provide benchmarks and evaluation scripts on GitHub to facilitate further development. We anticipate that these contributions will benefit researchers and practitioners alike, advancing this critical field toward more effective and accessible medical AI applications. Results Overall performance of medical text analysis compares the performance of our Me-LLaMA 13 / 70B foundation modelsagainst otheropen LLMsin the supervised setting. The performance of Meditron 70B on the PubMedQA, MedQA, and MedMCQA datasets is cited from the meditron paper to have a fair comparison. We can observe that the Me-LLaMA 13B model surpassed the similar-sized medical foun-dation model PMC-LLaMA 13B on 11 out of 12 datasets and outperformed the general foundation model LLaMA2 13B on 10 out of 12 datasets. Moreover, itisnoticedthattheMe-LLaMA13Bmodelwas","label":[[420,423,"MODEL"],[377,384,"MODEL"],[183,188,"MODEL"],[485,490,"MODEL"],[714,719,"MODEL"],[1081,1086,"MODEL"],[1600,1605,"MODEL"],[1845,1850,"MODEL"],[1919,1924,"MODEL"],[2000,2006,"MODEL"],[549,556,"MODEL"],[13,32,"TECHNIQUE"],[313,331,"TECHNIQUE"],[914,932,"TECHNIQUE"],[1252,1262,"TECHNIQUE"],[39,52,"APPLICATION"],[1741,1748,"DATASET"],[1720,1728,"DATASET"]],"Comments":[]}
