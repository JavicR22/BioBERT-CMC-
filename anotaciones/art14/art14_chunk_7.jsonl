{"id":7,"text":"141 2 paper1. We compared the Rouge-113 score for the summarization dataset PubMed, the accuracy score for three QA datasets, and the Macro-F1 score for the remaining datasets. With task-specific supervised fine-tuning, Me-LLaMA modelssurpassed ChatGPT on 7out of 8 datasets and excelled GPT-4 on 5 out of 8 datasets. In the zero-shot setting, Me-LLaMA models outperformed ChatGPT on 5 datasets ; but it fell short on 7 datasets, when compared with GPT-4. It ’ s crucial to highlight that Me-LLaMA ’ s model size issignificantlysmaller-13 / 70B parametersversusatleast175B forChatGPT and GPT-4. Despite this size discrepancy, Me-LLaMA models have show-cased an impressive performance and a strong ability for supervised learning The zero-shot performance of various open source LLMs with chat capability Task Dataset Metric LLaMA2-13B-chat PMC-LLaMA-chat Medalpaca-13B AlpaCare-13B Me-LLaMA 13B-chat LLaMA2-70B-chat Me-LLaMA 70B-chat Question answering PubMedQA Accuracy 0. 546 0. 504 0. 238 0. 538 0. 700 0. 668 0. 768 Macro-F1 0. 457 0. 305 0. 192 0. 373 0. 504 0. 477 0. 557 MedQA Accuracy 0. 097 0. 207 0. 143 0. 304 0. 427 0. 376 0. 523 Macro-F1 0. 148 0. 158 0. 102 0. 281 0. 422 0. 367 0. 521 MedMCQA Accuracy 0. 321 0. 212 0. 205 0. 385 0. 449 0. 339 0. 539 Macro-F1 0. 243 0. 216 0. 164 0. 358 0. 440 0. 273 0. 538 EmrQA Accuracy 0. 001 0. 053 0. 000 0. 001 0. 048 0.","label":[[288,291,"MODEL"],[449,452,"MODEL"],[588,591,"MODEL"],[245,252,"MODEL"],[373,380,"MODEL"],[855,864,"MODEL"],[223,228,"MODEL"],[347,352,"MODEL"],[492,497,"MODEL"],[629,634,"MODEL"],[844,849,"MODEL"],[885,890,"MODEL"],[919,924,"MODEL"],[824,830,"MODEL"],[709,728,"TECHNIQUE"],[207,218,"TECHNIQUE"],[720,728,"TECHNIQUE"],[88,96,"METRIC"],[962,970,"METRIC"],[1084,1092,"METRIC"],[1208,1216,"METRIC"],[1330,1338,"METRIC"],[30,35,"METRIC"],[934,952,"APPLICATION"],[54,67,"APPLICATION"],[76,82,"DATASET"],[1200,1207,"DATASET"],[953,961,"DATASET"]],"Comments":[]}
