{"id":9,"text":". 001 with an Adam optimizer. We iterated the training process for 20 epochs with batch size 64 and early stopped if the training loss did not decrease. For sentence similarity tasks, we used bag of embeddings with the average strategy to transform the sequence of word embeddings into a sentence embedding. Afterward, we concatenated two sen-tence embeddings and fed them into an architec-ture with one dense layer to predict the similarity of two sentences. 10https : / / allennlp. org / elmo 62 Task Metrics SOTA * ELMo BioBERT Our BERT Base Base Large Large ( P ) ( P + M ) ( P ) ( P + M ) MedSTS Pearson 83. 6 68. 6 84. 5 84. 5 84. 8 84. 6 83. 2 BIOSSES Pearson 84. 8 60. 2 82. 7 89. 3 91. 6 86. 3 75. 1 BC5CDR-disease F 84. 1 83. 9 85. 9 86. 6 85. 4 82. 9 83. 8 BC5CDR-chemical F 93. 3 91. 5 93. 0 93. 5 92. 4 91. 7 91. 1 ShARe / CLEFE F 70. 0 75. 6 72. 8 75. 4 77. 1 72. 7 74. 4 DDI F 72. 9 78. 9 78. 8 78. 1 79. 4 79. 9 76. 3 ChemProt F 64. 1 66. 6 71. 3 72. 5 69. 2 74. 4 65. 1 i2b2 F 73. 7 71. 2 72. 2 74. 4 76. 4 73. 3 73. 9 HoC F 81. 5 80. 0 82. 9 85. 3 83. 1 87. 3 85. 3 MedNLI acc 73. 5 71. 4 80. 5 82. 2 84. 0 81. 5 83. 8 Total 78. 8 80. 5 82. 2 82. 3 81. 5 79. 2 * SOTA, state-of-the-art as of April 2019, to the best of our knowledge : MedSTS, BIOSSES ( Chen, 2019 ) ; BC5CDR-disease, BC5CDR-chem ( Yoon","label":[[239,248,"MODEL"],[523,530,"MODEL"],[490,494,"MODEL"],[518,522,"MODEL"],[535,539,"MODEL"],[14,18,"TECHNIQUE"],[540,544,"TECHNIQUE"],[545,549,"TECHNIQUE"],[934,942,"DATASET"],[987,991,"DATASET"]],"Comments":[]}
