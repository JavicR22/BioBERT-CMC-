{"id":1,"text":"Publicly Available Clinical BERT Embeddings Emily Alsentzer Harvard-MIT Cambridge, MA emilya @ mit. edu John R. Murphy MIT CSAIL Cambridge, MA jrmurphy @ mit. edu Willie Boag MIT CSAIL Cambridge, MA wboag @ mit. edu Wei-Hung Weng MIT CSAIL Cambridge, MA ckbjimmy @ mit. edu Di Jin MIT CSAIL Cambridge, MA jindi15 @ mit. edu Tristan Naumann Microsoft Research Redmond, WA tristan @ microsoft. com Matthew B. A. McDermott MIT CSAIL Cambridge, MA mmd @ mit. edu Abstract Contextual word embedding models such as ELMo ( Peters, 2018 ) and BERT ( De-vlin, 2018 ) have dramatically improved performance for many natural language pro-cessing ( NLP ) tasks in recent months. How-ever, these models have been minimally ex-plored on specialty corpora, such as clini-cal text ; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text : one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domain-specific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthet-ically non de-identified task text. 1 Introduction Natural language processing ( NLP ) has been shaken in recent months with the dramatic suc-cesses enabled by transfer learning and contextual word embedding models, such as ELMo ( Peters, 2018 ), ULMFiT ( Howard and Ruder, 2018 ), and BERT ( Devlin, 2018 ). These models have been primarily explored for general domain text, and, recently, biomedical text with BioBERT ( Lee, 2019 ). However, clin-i","label":[[1802,1809,"MODEL"],[1637,1643,"MODEL"],[509,513,"MODEL"],[1614,1618,"MODEL"],[28,32,"MODEL"],[535,539,"MODEL"],[835,839,"MODEL"],[920,924,"MODEL"],[1676,1680,"MODEL"],[1441,1468,"TECHNIQUE"],[1550,1567,"TECHNIQUE"],[637,640,"TECHNIQUE"],[1139,1142,"TECHNIQUE"],[1471,1474,"TECHNIQUE"],[1130,1148,"APPLICATION"],[998,1017,"TECHNOLOGY"]],"Comments":[]}
