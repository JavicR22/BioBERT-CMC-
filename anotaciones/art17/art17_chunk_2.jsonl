{"id":2,"text":"##cal narratives ( e. g., physician notes ) have known differences in linguistic characteristics from both general text and non-clinical biomedical text, mo-tivating the need for specialized clinical BERT models. In this work, we build and publicly release ex-actly such an embedding model. 1 Furthermore, we demonstrate on several clinical NLP tasks the improvements this system offers over traditional BERT and BioBERT alike. In particular, we make the following contribu-tions : 1. We train and publicly release BERT-Base and BioBERT-finetuned models trained on both all clinical notes and only discharge sum-maries. 2 2. We demonstrate that using clinical specific contextual embeddings improves both upon general domain results and BioBERT results across 2 well established clinical NER tasks and one medical natural language inference task ( i2b2 2010 ( Uzuner, 2011 ), i2b2 2012 ( Sun, 2013a, b ), and MedNLI ( Romanov and Shivade, 2018 ) ). On 2 de-identification ( de-ID ) tasks, i2b2 2006 ( Uzuner, 2007 ) and i2b2 2014 ( Stubbs, 2015 ; Stubbs and Uzuner, 2015 ), gen-eral BERT and BioBERT outperform clinical BERT and we argue that fundamental facets of the de-ID context motivate this lack of per-formance. 2 Related Work Contextual Embeddings in General Tradi-tional word-level vector representations, such as word2vec ( Mikolov, 2013 ), GloVe ( Penning-ton, 2014 ), and fastText ( Bojanowski, 1github. com / EmilyAlsentzer / clinicalBERT 2Discharge summaries are commonly used in downstream tasks. arXiv : 1904. 03323v3 [ cs. CL ] 20 Jun 2019 2017 ), express all possible meanings of a word as a single vector representation and cannot disam-biguate the word senses based on the surround-ing context. Over the last two years, ELMo ( Pe-ters, 2018 ) and BERT ( Devlin, 2018 ) present strong","label":[[1439,1451,"MODEL"],[515,524,"MODEL"],[1323,1331,"MODEL"],[1384,1392,"MODEL"],[413,420,"MODEL"],[529,536,"MODEL"],[737,744,"MODEL"],[1092,1099,"MODEL"],[1351,1356,"MODEL"],[1740,1744,"MODEL"],[200,204,"MODEL"],[404,408,"MODEL"],[1083,1087,"MODEL"],[1120,1124,"MODEL"],[1767,1771,"MODEL"],[669,690,"TECHNIQUE"],[1234,1255,"TECHNIQUE"],[341,344,"TECHNIQUE"],[788,791,"TECHNIQUE"],[332,350,"APPLICATION"],[848,852,"DATASET"],[876,880,"DATASET"],[989,993,"DATASET"],[1020,1024,"DATASET"]],"Comments":[]}
