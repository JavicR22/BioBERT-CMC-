{"id":3,"text":"solutions that can provide contex-tualized word representations. By pre-training on a large text corpus as a language model, ELMo can create a context-sensitive embedding for each word in a given sentence, which will be fed into downstream tasks. Compared to ELMo, BERT is deeper and contains much more parameters, thus possessing greater representation power. More im-portantly, rather than simply providing word em-beddings as features, BERT can be incorporated into a downstream task and gets fine-tuned as an integrated task-specific architecture. BERT has, in general, been found to be supe-rior to ELMo and far superior to non-contextual embeddings on a variety of tasks, including those in the clinical domain ( Si, 2019 ). For this reason, we only examine BERT here, rather than including ELMo or non-contextual embed-ding methods. Contextual Clinical & Biomedical Embeddings Several works have explored the utility of con-textual models in the clinical and biomedical do-mains. BioBERT ( Lee, 2019 ) trains a BERT model over a corpus of biomedical research arti-cles sourced from PubMed3 article abstracts and PubMed Central4 article full texts. They find the specificity offered by biomedical texts trans-lated to improved performance on several biomed-ical NLP tasks, and fully release their pre-trained BERT model. On clinical text, ( Khin, 2018 ) uses a general-domain pretrained ELMo model towards the task of clinical text de-identification, report-ing near state-of-the-art performance on the i2b2 2014 task ( Stubbs and Uzuner, 2015 ; Stubbs, 2015 ) and state of the art performance on several axes of the HIPAA PHI dataset. Two works that we know of train contextual em-bedding models on clinical corpora. ( Zhu, 2018 ) trains an ELMo model over a corpus of mixed clinical discharge summaries, clinical radiology notes and medically oriented wikipedia articles, then demonstrates improved performance on the i2b2 2010 task ( Uzuner, 3https : / / www. n","label":[[987,994,"MODEL"],[125,129,"MODEL"],[259,263,"MODEL"],[604,608,"MODEL"],[797,801,"MODEL"],[1393,1397,"MODEL"],[1748,1752,"MODEL"],[265,269,"MODEL"],[439,443,"MODEL"],[552,556,"MODEL"],[764,768,"MODEL"],[1018,1022,"MODEL"],[1315,1319,"MODEL"],[633,654,"TECHNIQUE"],[1268,1271,"TECHNIQUE"],[1169,1180,"METRIC"],[1119,1125,"DATASET"],[1509,1513,"DATASET"],[1926,1930,"DATASET"],[1791,1810,"TECHNOLOGY"]],"Comments":[]}
