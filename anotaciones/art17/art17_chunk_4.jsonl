{"id":4,"text":"##cbi. nlm. nih. gov / pubmed / 4https : / / www. ncbi. nlm. nih. gov / pmc / 2011 ). They release a pre-trained ELMo model along with their work, enabling further clinical NLP research to work with these powerful con-textual embeddings. ( Si, 2019 ), released in late February 2019, train a clinical note corpus BERT language model and uses complex task-specific models to yield im-provements over both traditional embeddings and ELMo embeddings on the i2b2 2010 and 2012 tasks ( Sun, 2013b, a ) and the SemEval 2014 task 7 ( Pradhan, 2014 ) and 2015 task 14 ( El-hadad ) tasks, establishing new state-of-the-art results on all four corpora. However, this work neither releases their embeddings for the larger community nor examines the performance oppor-tunities offered by fine-tuning BioBERT with clin-ical text or by training note-type specific embed-ding models, as we do. 3 Methods In this section, we first describe our clinical text dataset, the details of the BERT training proce-dure, and finally the specific tasks we examine. 3. 1 Data We use clinical text from the approximately 2 mil-lion notes in the MIMIC-III v1. 4 database ( John-son, 2016 ). Details of our text pre-processing procedure can be found in Appendix A. Note that while some of our tasks use a small subset of MIMIC notes in their corpora, we do not try to fil-ter these notes out of our BERT pre-training proce-dure. We expect the bias this induces is negligible given the relative sizes of the two corpora. We train two varieties of BERT on MIMIC notes : Clinical BERT, which uses text from all note types, and Discharge Summary BERT, which uses only discharge summaries in an effort to tai-lor the corpus to downstream tasks ( which often largely use discharge summaries ). Note that we train our clinical B","label":[[788,795,"MODEL"],[342,349,"MODEL"],[113,117,"MODEL"],[431,435,"MODEL"],[313,317,"MODEL"],[970,974,"MODEL"],[1369,1373,"MODEL"],[1516,1520,"MODEL"],[1547,1551,"MODEL"],[1612,1616,"MODEL"],[1182,1196,"TECHNIQUE"],[776,787,"TECHNIQUE"],[173,176,"TECHNIQUE"],[1342,1345,"METRIC"],[1117,1126,"DATASET"],[23,29,"DATASET"],[454,458,"DATASET"],[1634,1653,"TECHNOLOGY"],[1735,1754,"TECHNOLOGY"]],"Comments":[]}
