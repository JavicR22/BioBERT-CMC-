{"id":5,"text":"##ERT instantia-tions on all notes of the appropriate type ( s ), with-out regard for whether or not any individual note appeared in any of the train / test sets for the vari-ous tasks we use ( two of which use a small subset of MIMIC notes either partially or completely as their backing corpora ). We feel this has a negligi-ble impact given the dramatically larger size of the entire MIMIC corpus relative to the various task corpora. 3. 2 BERT Training In this work, we aim to provide the pre-trained embeddings as a community resource, rather than demonstrate technical novelty in the training pro-cedure, and accordingly our BERT training pro-cedure is completely standard. As such, we have relegated specifics of the training procedure to Ap-pendix B. We trained two BERT models on clinical text : 1 ) Clinical BERT, initialized from BERT-Base, and 2 ) Clinical BioBERT, initialized from BioBERT. For all downstream tasks, BERT mod-els were allowed to be fine-tuned, then the out-put BERT embedding was passed through a single linear layer for classification, either at a per-token level for NER or de-ID tasks or applied to the sen-tinel \" begin sentence \" token for MedNLI. Note that this is a substantially lower capacity model than, for example, the Bi-LSTM layer used in ( Si, 2019 ). This reduced capacity potentially limits performance on downstream tasks, but is in line with our goal of demonstrating the efficacy of clinical-specific embeddings and releasing a pre-trained BERT model for these embeddings. We did not experiment with more complex representa-tions as our goal is not to necessarily surpass state-of-the-art performances on these tasks. Computational Cost Pre-processing and train-ing BERT on MIMIC notes took significant com-putational resources. We estimate that our en-tire embedding model procedure took roughly 17-18 days of computational runtime using a single GeForce GTX TITAN X 12 GB GPU ( and signifi","label":[[841,850,"MODEL"],[869,876,"MODEL"],[895,902,"MODEL"],[1555,1562,"MODEL"],[1261,1268,"MODEL"],[443,447,"MODEL"],[631,635,"MODEL"],[774,778,"MODEL"],[818,822,"MODEL"],[930,934,"MODEL"],[991,995,"MODEL"],[1490,1494,"MODEL"],[1716,1720,"MODEL"],[1687,1701,"TECHNIQUE"],[1099,1102,"TECHNIQUE"]],"Comments":[]}
