{"id":8,"text":"Operations Generic Glucose Seizure Pneumonia Transfer Admitted Discharge Beach Newspaper Table BioBERT insulin episode vaccine drainage admission admission coast news tables exhaustion appetite infection division sinking wave rock official row dioxide attack plague transplant hospital sight reef industry dinner Clinical potassium headache consolidation transferred admission disposition shore publication scenario sodium stroke tuberculosis admitted transferred transfer ocean organization compilation sugar agitation infection arrival admit transferred land publicity technology Nearest neighbors for 3 sentinel words for each of 3 categories. In the Disease and operations categories, clinical BERT appears to show greater cohesion within the clinical domain than BioBERT, whereas for generic words, the methods do not differ much, as expected. ture of de-ID challenges. De-ID challenge data presents a different data distribution than MIMIC text. In MIMIC, PHI is identified and replaced with sentinel PHI mark-ers, whereas in the de-ID task, PHI is masked with synthetic, but realistic PHI. This data drift would be problematic for any embedding model, but will be especially damaging to contextual embedding models like BERT because the underlying sen-tence structure will have changed : in raw MIMIC, sentences with PHI will universally have a sentinel PHI token. In contrast, in the de-ID corpus, all such sentences will have different synthetic masks, meaning that a canonical, nearly constant sentence structure present during BERT â€™ s training will be non-existent at task-time. For these reasons, we think it is sensible that clinical BERT is not suc-cessful on the de-ID corpora. Furthermore, this is a good example for the community given how prevalent the assumption is that contextual embed-ding models trained on task-like corpora will offer dramatic improvements. Overall, we feel our results demonstrates the utility of using domain-specific contextual embed-dings for non de-ID clinical NLP tasks. Addition-ally, on one task Discharge Summary BERT offers performance improvements over Clinical BERT, so it may be that adding greater specificity to the underlying corpus is helpful in some cases. We release both models with this work for public use. Qualitative Embedding Comparisons shows the nearest neighbors for 3 words","label":[[698,702,"MODEL"],[1227,1231,"MODEL"],[1538,1542,"MODEL"],[1648,1652,"MODEL"],[2064,2068,"MODEL"],[2115,2119,"MODEL"],[95,102,"MODEL"],[768,775,"MODEL"],[1142,1157,"MODEL"],[1205,1221,"MODEL"],[2154,2165,"METRIC"]],"Comments":[]}
