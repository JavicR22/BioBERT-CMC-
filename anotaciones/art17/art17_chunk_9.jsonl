{"id":9,"text":"each from 3 categories under BioBERT and Clinical BERT. These lists suggest that Clinical BERT re-tains greater cohesion around medical or clinic-operations relevant terms than does BioBERT. For example, the word \" Discharge \" is most closely associated with \" admission, \" \" wave, \" and \" sight \" under BioBERT, yet only the former seems rele-vant to clinical operations. In contrast, under Clin-ical BERT, the associated words all are meaningful in a clinical operations context. Limitations & Future Work This work has several notable limitations. First, we do not ex-periment with any more advanced model architec-tures atop our embeddings. This likely hurts our performance. Second, MIMIC only contains notes from the intensive care unit of a single healthcare institution ( BIDMC ). Differences in care practices across institutions are significant, and using notes from multiple institutions could offer significant gains. Lastly, our model shows no improvements for either de-ID task we explored. If our hypoth-esis is correct as to its cause, a possible solution could entail introducing synthetic de-ID into the source clinical text and using that as the source for de-ID tasks going forward. 5 Conclusion In this work, we pretrain and release clinically ori-ented BERT models, some trained solely on clini-cal text, and others fine-tuned atop BioBERT. We find robust evidence that our clinical embeddings are superior to general domain or BioBERT spe-cific embeddings for non de-ID tasks, and that us-ing note-type specific corpora can induce further selective performance benefits. To the best of our knowledge, our work is the first to release clini-cally trained BERT models. Our hope is that all clinical NLP researchers will be able to benefit from these embeddings without the necessity of the significant computational resources required to train these models over the MIMIC corpus. 6","label":[[29,36,"MODEL"],[182,189,"MODEL"],[304,311,"MODEL"],[1354,1361,"MODEL"],[1450,1457,"MODEL"],[50,54,"MODEL"],[90,94,"MODEL"],[402,406,"MODEL"],[1275,1279,"MODEL"],[1677,1681,"MODEL"],[1720,1723,"TECHNIQUE"],[276,280,"TECHNOLOGY"]],"Comments":[]}
