{"id":1,"text":"arXiv : 1903. 10676v3 [ cs. CL ] 10 Sep 2019 SCIBERT : A Pretrained Language Model for Scientific Text Iz Beltagy Kyle Lo Arman Cohan Allen Institute for Artificial Intelligence, Seattle, WA, USA { beltagy, kylel, armanc } @ allenai. org Abstract Obtaining large-scale annotated data for NLP tasks in the scientific domain is challeng-ing and expensive. We release SCIBERT, a pretrained language model based on BERT ( Devlin, 2019 ) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve perfor-mance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demon-strate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https : / / github. com / allenai / scibert /. 1 Introduction The exponential increase in the volume of scien-tific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these docu-ments. Recent progress in NLP has been driven by the adoption of deep neural models, but train-ing such models often requires large amounts of labeled data. In general domains, large-scale train-ing data is often possible to obtain through crowd-sourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the ex-pertise required for quality annotation. As shown through ELMo ( Peters, 2018 ), GPT ( Radford, 2018 ) and BERT ( Devlin, 2019 ), unsupervised pre-training of language models on large corpora significantly improves performance on many NLP tasks. These","label":[[411,415,"MODEL"],[888,892,"MODEL"],[1717,1721,"MODEL"],[45,52,"MODEL"],[365,372,"MODEL"],[507,514,"MODEL"],[1045,1052,"MODEL"],[1691,1694,"MODEL"],[756,774,"TECHNIQUE"]],"Comments":[]}
