{"id":2,"text":"models return contextualized embeddings for each token which can be passed into minimal task-specific neural architectures. Leveraging the success of unsupervised pretrain-ing has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-tions : ( i ) We release SCIBERT, a new resource demon-strated to improve performance on a range of NLP tasks in the scientific domain. SCIBERT is a pre-trained language model based on BERT but trained on a large corpus of scientific text. ( ii ) We perform extensive experimentation to investigate the performance of finetuning ver-sus task-specific architectures atop frozen embed-dings, and the effect of having an in-domain vo-cabulary. ( iii ) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-of-the-art ( SOTA ) results on many of these tasks. 2 Methods Background The BERT model architecture ( Devlin, 2019 ) is based on a multilayer bidi-rectional Transformer ( Vaswani, 2017 ). In-stead of the traditional left-to-right language mod-eling objective, BERT is trained on two tasks : pre-dicting randomly masked tokens and predicting whether two sentences follow each other. SCIB-ERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece ( Wu, 2016 ) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with BERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vo-cabulary on our scientific corpus using the Sen-tence","label":[[1187,1198,"MODEL"],[314,318,"MODEL"],[677,681,"MODEL"],[1106,1110,"MODEL"],[1290,1294,"MODEL"],[1454,1458,"MODEL"],[1516,1520,"MODEL"],[1743,1747,"MODEL"],[517,524,"MODEL"],[628,635,"MODEL"],[953,960,"MODEL"],[1751,1760,"MODEL"],[1775,1783,"MODEL"],[1566,1578,"TECHNIQUE"],[810,820,"TECHNIQUE"]],"Comments":[]}
