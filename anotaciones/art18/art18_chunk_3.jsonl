{"id":3,"text":"##Piece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The re-sulting token overlap between BASEVOCAB and SCIVOCAB is 42 %, illustrating a substantial dif-ference in frequently used words between scien-tific and general domain texts. Corpus We train SCIBERT on a random sample of 1. 14M papers from Semantic Scholar ( Ammar, 2018 ). This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences ( 2, 769 tokens ) resulting in a corpus size of 3. 17B tokens, similar to the 3. 3B tokens on which BERT was trained. We split sentences using ScispaCy ( Neumann, 2019 ), 2 which is optimized for scientific text. 3 Experimental Setup 3. 1 Tasks We experiment on the following core NLP tasks : 1. Named Entity Recognition ( NER ) 2. PICO Extraction ( PICO ) 3. Text Classification ( CLS ) 4. Relation Classification ( REL ) 5. Dependency Parsing ( DEP ) PICO, like NER, is a sequence labeling task where the model extracts spans describing the Partici-pants, Interventions, Comparisons, and Outcomes in a clinical trial paper ( Kim, 2011 ). REL is a special case of text classification where the model predicts the type of relation expressed be-tween two entities, which are encapsulated in the sentence by inserted special tokens. 3. 2 Datasets For brevity, we only describe the newer datasets here, and refer the reader to the","label":[[121,130,"MODEL"],[169,178,"MODEL"],[767,775,"MODEL"],[183,191,"MODEL"],[327,334,"MODEL"],[724,728,"MODEL"],[984,1003,"TECHNIQUE"],[1289,1308,"TECHNIQUE"],[1050,1068,"TECHNIQUE"],[1098,1115,"TECHNIQUE"],[956,971,"TECHNIQUE"],[905,908,"TECHNIQUE"],[947,950,"TECHNIQUE"],[1088,1091,"TECHNIQUE"]],"Comments":[]}
