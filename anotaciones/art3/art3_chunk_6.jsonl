{"id":6,"text":"##ive models ( text vectorisation step in TF-IDF measures a word ’ s importance in a text by computing its term frequency, indicating the word ’ s rela-tive frequency in a document. 19 This method is effec-tive for assessing word relevance in document queries. Word2Vec vectorises text using a neural network to create word embeddings, mapping words to vectors. 20 It employs a sliding window technique, using either the continuous bag-of-words ( CBOW ) method to predict a word from its context or the skip-gram method to predict context words from a given word. Doc2Vec, a generalised Word2Vec, vectorises entire paragraphs or documents directly into single vectors, bypassing the averaging step required in Word2Vec. 21 It offers two algorithms : distributed memory ( DM ) and distributed bag of words ( DBOW ). 22 shows the Word2Vec and Doc2Vec algorithms. Predictive modelling After the texts were vectorised, the rows of numerically encoded features for both the training and test sets were prepared. We performed feature selection to filter out features that did not positively contribute to the classifi-cation task. This step further reduced the feature dimen-sion, resulting in a more compact space. We trained a random forest ( RF ) classifier to determine the top rele-vant features for each text vectoriser ( feature selection step in The transformed training set was used to train the predictive models using multiple classification methods ( classification step in We used three different classification approaches : support vector classification ( SVC ), K-nearest neighbours ( KNN ) and RF. These approaches spanned a wide variety of classifier catego-ries, including support vector machines, non-parametric methods and ensemble methods, enabling us to evaluate a broader spectrum of model performance. All of these methods were supervised learning techniques ; therefore, we used the annotated training set, composed of 70 clin-ical notes, to train each model. The SVC finds a hyperplane that maximises the margin between the nearest data points of each label, with hyper-parameters tuned for optimal separation. 23 KNN classifies by voting among the ‘ k ’ nearest training data points to an input query, leading to larger models with more data. 24 25 RF, an ensemble of decision trees, combines their predic-tions to reduce","label":[[1399,1416,"MODEL"],[2234,2247,"MODEL"],[1223,1236,"MODEL"],[1352,1363,"MODEL"],[261,269,"MODEL"],[587,595,"MODEL"],[710,718,"MODEL"],[828,836,"MODEL"],[1594,1597,"MODEL"],[2134,2137,"MODEL"],[1020,1037,"TECHNIQUE"],[1322,1339,"TECHNIQUE"],[20,33,"TECHNIQUE"],[432,444,"TECHNIQUE"],[1857,1865,"TECHNIQUE"],[42,48,"TECHNIQUE"]],"Comments":[]}
