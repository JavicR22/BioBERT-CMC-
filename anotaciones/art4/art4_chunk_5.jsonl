{"id":5,"text":"##TM ) network ( instead of learning fixed word representations ) and then fine-tuning the biLSTM network according to specific downstream tasks. Furthermore, based on the highly parallelizable Transformer architecture with self-attention mechanisms, BERT was proposed by pre-training bidirectional language models with specially designed pre-training tasks on large-scale unlabeled cor-pora. These pre-trained context-aware word representations are very effective as general-purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow-up work, which sets the \" pre-training and fine-tuning \" learning paradigm. Following this paradigm, a great number of stud-ies on PLMs have been developed, introducing either differ-ent architectures ( e. g., GPT-2 and BART ) or improved pre-training strategies [ 27 – 29 ]. In this paradigm, it often requires fine-tuning the PLM for adapting to different downstream tasks. Large language models ( LLM ). Researchers find that scaling PLM ( e. g., scaling model size or data size ) often leads to an improved model capacity on downstream tasks ( i. e., following the scaling law ). A number of studies 3 have explored the performance limit by training an ever larger PLM ( e. g., the 175B-parameter GPT-3 and the 540B-parameter PaLM ). Although scaling is mainly conducted in model size ( with similar architectures and pre-training tasks ), these large-sized PLMs display different behaviors from smaller PLMs ( e. g., 330M-parameter BERT and 1. 5B-parameter GPT-2 ) and show surprising abilities ( called emer-gent abilities ) in solving a series of complex tasks. For example, GPT-3 can solve few-shot tasks through in-context learning, whereas GPT-2 cannot do well. Thus, the research community coins the term \" large language models ( LLM ) \" 1 for these large-sized PLMs [ 32 – 35 ], which attract increasing research attention ( See A remarkable application of LLMs is ChatGPT2 that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the arXiv","label":[[91,97,"MODEL"],[194,205,"MODEL"],[251,255,"MODEL"],[1549,1553,"MODEL"],[822,825,"MODEL"],[1313,1316,"MODEL"],[1574,1577,"MODEL"],[1694,1697,"MODEL"],[1762,1765,"MODEL"],[2029,2032,"MODEL"],[822,827,"MODEL"],[1574,1579,"MODEL"],[1762,1767,"MODEL"],[1313,1318,"MODEL"],[1694,1699,"MODEL"],[1990,1998,"MODEL"],[832,836,"MODEL"],[1666,1673,"MODEL"],[1733,1752,"TECHNIQUE"],[75,86,"TECHNIQUE"],[655,666,"TECHNIQUE"],[924,935,"TECHNIQUE"],[229,249,"TECHNIQUE"],[224,238,"TECHNIQUE"],[28,36,"TECHNIQUE"]],"Comments":[]}
