{"id":23,"text":"the concept does not appear in background knowledge ; ( ii ) the list con-tains only a single term ; or ( iii ) the list encompasses multiple terms. In the second case, a direct mapping is established. In contrast, for the last scenario in our methodology, we uti-lize an LLM to facilitate the determination of the optimal solution, taking into account contextual nuances, the range of ranked candidates, and the definition of the concept in the specific vocabulary. 4 https : / / docs. python. org / 3 / library / difflib. html 123 Medical & Biological Engineering & Computing 63 : 749 – 772 761 4. 4 Medical entity disambiguation An LLM is used by NSSC to produce a result, considering the contextual information provided by the previous phase. This model offers insights into likely meanings based on the context in which the term is used. Creating a robust context is key to improving the performance of language models in disambiguation tasks. A detailed explanation of this process can be found in Algorithm 3. When using LLMs, several techniques can be used to control the behavior of the model and achieve the desired results. One important method is prompt engineering, which involves creating spe-cific prompts to guide the model ’ s behavior. These prompts may include explicit instructions, contextual setting, or spe-cial formatting to guide the model towards the expected output. Two important prompt engineering techniques are as follows : Few-shot learning involves giving the model a limited set of examples or demonstrations that illus-trate the target behavior. This method helps the model better understand the intended task or concept more effectively. The model then uses these examples to gen-eralize and formulate responses to novel prompts or questions, demonstrating understanding beyond the spe-cific instances provided. Chain of Thought ( CoT ) involves crafting prompts that encourage the model to reveal its reasoning process in a step-by-step manner, similar to how a human might think aloud while solving a problem. This approach is particularly effective for complex tasks, because it seeks not only the correct answer but also the logical path to that answer. By breaking down the thought process, CoT enhances the model ’ s ability to handle complex ques-tions and provides users with","label":[[2092,2099,"MODEL"],[2277,2284,"MODEL"],[1455,1472,"TECHNIQUE"],[1159,1177,"TECHNIQUE"],[1408,1426,"TECHNIQUE"],[1464,1472,"TECHNIQUE"],[386,392,"APPLICATION"]],"Comments":[]}
