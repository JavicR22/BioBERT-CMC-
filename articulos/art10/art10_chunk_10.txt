using both the PubMed and PMC corpora, we found that 200K and 270K pre - training steps were optimal for PubMed and PMC, respectively. We also used the ablated versions of BioBERT v1. 0, which were pre - trained on only PubMed for 200K steps ( BioBERT v1. 0 ( þ PubMed ) ) and PMC for 270K steps ( BioBERT v1. 0 ( þ PMC ) ). After our initial release of BioBERT v1. 0, we pre - trained BioBERT on PubMed for 1M steps, and we refer to this version as BioBERT v1. 1 ( þ PubMed ). Other hyper - parameters such as batch size and learning rate scheduling for pre - training BioBERT are the same as those for pre - training BERT unless stated otherwise. We pre - trained BioBERT using Naver Smart Machine Learning ( NSML ) ( Sung, 2017 ), which is utilized for large - scale experi - ments that need to be run on several GPUs. We used eight NVIDIA V100 ( 32GB ) GPUs for the pre - training. The maximum sequence length was fixed to 512 and the mini - batch size was set to 192, resulting in 98 304 words per iteration. It takes more than 10 days to pre - train BioBERT v1. 0 ( þ PubMed þ PMC ) nearly 23 days for BioBERT v1. 1 ( þ PubMed ) in this setting. Despite our best efforts to use BERTLARGE, we used only BERTBASE due to the computational complexity of BERTLARGE. We used a single NVIDIA Titan Xp ( 12GB ) GPU to fine - tune BioBERT on each task. Note that the fine - tuning process is more computationally efficient than pre - training BioBERT. For fine - tuning, a batch size of 10, 16, 32 or 64 was selected, and a learning rate of 5e5, 3e5 or 1e5 was selected. Fine - tuning BioBERT on QA and RE tasks took less than an hour as the size of the training data is much smaller than that of the training data used by Devlin. On the other