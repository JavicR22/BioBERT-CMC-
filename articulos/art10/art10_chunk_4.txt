pre - trained on biomedical domain corpora ( PubMed abstracts and PMC full - text articles ). To show the ef - fectiveness of our approach in biomedical text mining, BioBERT is fine - tuned and evaluated on three popular biomedical text mining tasks ( NER, RE and QA ). We test various pre - training strategies with differ - ent combinations and sizes of general domain corpora and biomedical corpora, and analyze the effect of each corpus on pre - training. We also provide in - depth analyses of BERT and BioBERT to show the necessity of our pre - training strategies. The contributions of our paper are as follows : BioBERT is the ﬁrst domain - speciﬁc BERT based model pre - trained on biomedical corpora for 23 days on eight NVIDIA V100 GPUs. We show that pre - training BERT on biomedical corpora largely improves its performance. BioBERT obtains higher F1 scores in biomedical NER ( 0. 62 ) and biomedical RE ( 2. 80 ), and a higher MRR score ( 12. 24 ) in biomedical QA than the current state - of - the - art models. Compared with most previous biomedical text mining models that are mainly focused on a single task such as NER or QA, our model BioBERT achieves state - of - the - art performance on various biomedical text mining tasks, while requiring only minimal architectural modiﬁcations. We make our pre - processed datasets, the pre - trained weights of BioBERT and the source code for ﬁne - tuning BioBERT publicly available. 3 Materials and methods BioBERT basically has the same structure as BERT. We briefly dis - cuss the recently proposed BERT, and then we describe in detail the pre - training and fine - tuning process of BioBERT. 3. 1 BERT : bidirectional encoder representations from transformers Learning word representations from a large amount of unannotated text is a long - established method. While previous models ( e. g. Word2Vec ( Mikolov, 2013 ), GloVe ( Pennington, 2014 ) ) focused on learning context independent word representations, re - cent works have focused on learning context dependent word