##competitivewith LLaMA2 70B and Meditron 70B, which have signiﬁcantly larger parameter sizes, on 8 out of 12 datasets. As for 70B models, Me - LLaMA 70B achieved the best performance on 9 out of 12 datasets, when benchmarked against LLaMA2 70B and Meditron 70B. shows the zero - shot performance of Me - LLaMA chat models and other instruction - tuned open LLMs with chat ability on various tasks. Among 13B models, Me - LLaMA 13B - chat outperformed LLaMA2 13B - chat, PMC - LLaMA - chat, Medalpaca 13B in almost all 12 datasets. Me - LLaMA outperformed AlpaCare - 13B in 9 out of 12 datasets. Among models with 70B parameters, Me - LLaMA 70B - chat consistently outperformed LLaMA2 - 70B - chat on 11 out of 12 datasets. It is worth noting that Me - LLaMA13B - chat showed better performance than LLaMA2 - 70B - chat — a model with a signiﬁcantly larger parameter size — on 6outof12datasetsandwascompetitivewiththeLLaMA2 - 70B - chatin3out of 6 remaining datasets. zero - shot and supervised learning setting, against ChatGPT and GPT - 4. Due to privacy concerns, which preclude the transmission of clinical datasets with patient information to ChatGPT and GPT - 4, we conducted our comparison across 8 datasets that are not subject to these limitations. The results of ChatGPTandGPT - 4onthreeQAdatasetsarereferencedfromtheOpenAI ’ s The supervised ﬁne - tuning performance of various open source LLMs on six tasks Task Dataset Metric LLaMA2 13B PMC - LLaMA 13B Me - LLaMA 13B LLaMA2 70B Meditron 70B Me - LLaMA 70B Question answering PubMedQA Acc 0. 800 0. 778 0. 802 0. 800 0. 800 0. 814 Macro - F1 0. 560 0. 544 0. 562 0. 560 – 0. 572 MedQA Acc 0. 467 0. 45