recognition, text classiﬁcation, text summarization, and natural language inference — across twelve datasets from both biomedical and clinical domains. Our results demonstrate that Me - LLaMA not only surpasses existing open - source medical LLMs in both zero - shot and supervised settings but also, with task - speciﬁc instruction tuning, outperforms leading commercial LLMs such as ChatGPT on seven out of eight datasets and GPT - 4 on ﬁve out of eight datasets. Furthermore, to evaluate Me - LLaMA ’ s potential clinical utility, we assessed the models on complex clinical case diagnosis tasks, comparing their performance with other commercial LLMs using both automatic and human evaluations. Ourﬁndingsindicatethat Me - LLaMA ’ s performance is comparabletothatofChatGPTandGPT - 4, despitetheirsubstantiallylarger model sizes. Our ﬁndings underscore the importance of combining domain - speciﬁc continual pretraining with instruction tuning to develop effective large language models for the medical domain. Recognizing the signiﬁcant resources required, we have publicly released our Me - LLaMA models on PhysioNet under appropriate Data Use Agreements ( DUAs ) to lower bar - riersandfosterinnovationwithinthemedicalAIcommunity. Alongsidethe models, we provide benchmarks and evaluation scripts on GitHub to facilitate further development. We anticipate that these contributions will beneﬁt researchers and practitioners alike, advancing this critical ﬁeld toward more effective and accessible medical AI applications. Results Overall performance of medical text analysis compares the performance of our Me - LLaMA 13 / 70B foundation modelsagainst otheropen LLMsin the supervised setting. The performance of Meditron 70B on the PubMedQA, MedQA, and MedMCQA datasets is cited from the meditron paper to have a fair comparison. We can observe that the Me - LLaMA 13B model surpassed the similar - sized medical foun - dation model PMC - LLaMA 13B on 11 out of 12 datasets and outperformed the general foundation model LLaMA2 13B on 10 out of 12 datasets. Moreover, itisnoticedthattheMe - LLaMA13Bmodelwas