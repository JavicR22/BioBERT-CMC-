050 0. 119 F1 0. 098 0. 304 0. 040 0. 198 0. 307 0. 251 0. 346 Named entity recognition i2b2 Macro - F1 0. 143 0. 091 0. 000 0. 173 0. 166 0. 321 0. 329 Relation extraction DDI Macro - F1 0. 090 0. 147 0. 058 0. 110 0. 214 0. 087 0. 283 Classiﬁcation HoC Macro - F1 0. 228 0. 184 0. 246 0. 267 0. 335 0. 309 0. 544 MTsample Macro - F1 0. 133 0. 083 0. 003 0. 273 0. 229 0. 254 0. 384 Summarization PubMed Rouge - L 0. 161 0. 028 0. 014 0. 167 0. 116 0. 192 0. 169 BERTS 0. 671 0. 128 0. 117 0. 671 0. 445 0. 684 0. 678 MIMIC - CXR Rouge - L 0. 144 0. 139 0. 010 0. 134 0. 400 0. 131 0. 418 BERTS 0. 704 0. 694 0. 502 0. 702 0. 797 0. 696 0. 787 Natural language inference BioNLI Macro - F1 0. 173 0. 159 0. 164 0. 170 0. 195 0. 297 0. 436 MedNLI Macro - F1 0. 412 0. 175 0. 175 0. 275 0. 472 0. 515 0. 675 Performance comparison of Me - LLaMA models with ChatGPT and GPT - 4. The ﬁgure presents the zero - shot performance of Me - LLaMA ( Me - LLaMA zero - shot ) alongside its supervised learning performance ( Me - LLaMA task - speciﬁc ), compared against the zero - shot performance of ChatGPT and GPT - 4 across 8 datasets. https : / / doi. org / 10. 1038 / s41746 - 025 - 01533 - 1 Article npj Digital Medicine | 8 : 141 3 and zero - shot learning across a broad spectrum of medical tasks, under - scoring its efﬁciency and potential