arXiv:1903.10676v3 [cs.CL] 10 Sep 2019 SCIBERT: A Pretrained Language Model for Scientiﬁc Text Iz Beltagy Kyle Lo Arman Cohan Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA {beltagy,kylel,armanc}@allenai.org Abstract Obtaining large-scale annotated data for NLP tasks in the scientiﬁc domain is challeng- ing and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin , 2019) to address the lack of high-quality, large-scale labeled scientiﬁc data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientiﬁc publications to improve perfor- mance on downstream scientiﬁc NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classiﬁcation and dependency parsing, with datasets from a variety of scientiﬁc domains. We demon- strate statistically signiﬁcant improvements over BERT and achieve new state-of-the- art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/. 1 Introduction The exponential increase in the volume of scien- tiﬁc publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these docu- ments. Recent progress in NLP has been driven by the adoption of deep neural models, but train- ing such models often requires large amounts of labeled data. In general domains, large-scale train- ing data is often possible to obtain through crowd- sourcing, but in scientiﬁc domains, annotated data is difﬁcult and expensive to collect due to the ex- pertise required for quality annotation. As shown through ELMo (Peters , 2018), GPT (Radford , 2018) and BERT (Devlin , 2019), unsupervised pre- training of language models on large corpora signiﬁcantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed into minimal task-speciﬁc neural architectures. Leveraging the success of unsupervised pretrain- ing has become especially important especially when task-speciﬁc annotations are difﬁcult to obtain, like in scientiﬁc NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu- tions: (i) We release SCIBERT, a new resource demon- strated to improve performance on a range of NLP tasks in the scientiﬁc domain. SCIBERT is a pre- trained language model based on BERT but trained on a large corpus of scientiﬁc text. (ii) We perform extensive experimentation to investigate the performance of ﬁnetuning ver- sus task-speciﬁc architectures atop frozen embed- dings, and the effect of having an in-domain vo- cabulary. (iii) We evaluate SCIBERT on a suite of tasks in the scientiﬁc domain, and achieve new state-of- the-art (SOTA) results on many of these tasks. 2 Methods Background The BERT model architecture (Devlin , 2019) is based on a multilayer bidi- rectional Transformer (Vaswani , 2017). In- stead of the traditional left-to-right language mod- eling objective, BERT is trained on two tasks: pre- dicting randomly masked tokens and predicting whether two sentences follow each other. SCIB- ERT follows the same architecture as BERT but is instead pretrained on scientiﬁc text. Vocabulary BERT uses WordPiece (Wu , 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with BERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vo- cabulary on our scientiﬁc corpus using the Sen- tencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The re- sulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial dif- ference in frequently used words between scien- tiﬁc and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar , 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained. We split sentences using ScispaCy (Neumann , 2019),2 which is optimized for scientiﬁc text. 3 Experimental Setup 3.1 Tasks We experiment on the following core NLP tasks: 1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classiﬁcation (CLS) 4. Relation Classiﬁcation (REL) 5. Dependency Parsing (DEP) PICO, like NER, is a sequence labeling task where the model extracts spans describing the Partici- pants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim , 2011). REL is a special case of text classiﬁcation where the model predicts the type of relation expressed be- tween two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets For brevity, we only describe the newer datasets here, and refer the reader to the