Publicly Available Clinical BERT Embeddings Emily Alsentzer Harvard-MIT Cambridge, MA emilya@mit.edu John R. Murphy MIT CSAIL Cambridge, MA jrmurphy@mit.edu Willie Boag MIT CSAIL Cambridge, MA wboag@mit.edu Wei-Hung Weng MIT CSAIL Cambridge, MA ckbjimmy@mit.edu Di Jin MIT CSAIL Cambridge, MA jindi15@mit.edu Tristan Naumann Microsoft Research Redmond, WA tristan@microsoft.com Matthew B. A. McDermott MIT CSAIL Cambridge, MA mmd@mit.edu Abstract Contextual word embedding models such as ELMo (Peters , 2018) and BERT (De- vlin , 2018) have dramatically improved performance for many natural language pro- cessing (NLP) tasks in recent months. How- ever, these models have been minimally ex- plored on specialty corpora, such as clini- cal text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries speciﬁcally. We demonstrate that using a domain-speciﬁc model yields performance improvements on three common clinical NLP tasks as compared to nonspeciﬁc embeddings. These domain- speciﬁc models are not as performant on two clinical de-identiﬁcation tasks, and argue that this is a natural consequence of the differences between de-identiﬁed source text and synthet- ically non de-identiﬁed task text. 1 Introduction Natural language processing (NLP) has been shaken in recent months with the dramatic suc- cesses enabled by transfer learning and contextual word embedding models, such as ELMo (Peters , 2018), ULMFiT (Howard and Ruder, 2018), and BERT (Devlin , 2018). These models have been primarily explored for general domain text, and, recently, biomedical text with BioBERT (Lee , 2019). However, clin- ical narratives (e.g., physician notes) have known differences in linguistic characteristics from both general text and non-clinical biomedical text, mo- tivating the need for specialized clinical BERT models. In this work, we build and publicly release ex- actly such an embedding model.1 Furthermore, we demonstrate on several clinical NLP tasks the improvements this system offers over traditional BERT and BioBERT alike. In particular, we make the following contribu- tions: 1. We train and publicly release BERT-Base and BioBERT-ﬁnetuned models trained on both all clinical notes and only discharge sum- maries.2 2. We demonstrate that using clinical speciﬁc contextual embeddings improves both upon general domain results and BioBERT results across 2 well established clinical NER tasks and one medical natural language inference task (i2b2 2010 (Uzuner , 2011), i2b2 2012 (Sun , 2013a,b), and MedNLI (Romanov and Shivade, 2018)). On 2 de-identiﬁcation (de-ID) tasks, i2b2 2006 (Uzuner , 2007) and i2b2 2014 (Stubbs , 2015; Stubbs and Uzuner, 2015), gen- eral BERT and BioBERT outperform clinical BERT and we argue that fundamental facets of the de-ID context motivate this lack of per- formance. 2 Related Work Contextual Embeddings in General Tradi- tional word-level vector representations, such as word2vec (Mikolov , 2013), GloVe (Penning- ton , 2014), and fastText (Bojanowski , 1github.com/EmilyAlsentzer/clinicalBERT 2Discharge summaries are commonly used in downstream tasks. arXiv:1904.03323v3 [cs.CL] 20 Jun 2019 2017), express all possible meanings of a word as a single vector representation and cannot disam- biguate the word senses based on the surround- ing context. Over the last two years, ELMo (Pe- ters , 2018) and BERT (Devlin , 2018) present strong solutions that can provide contex- tualized word representations. By pre-training on a large text corpus as a language model, ELMo can create a context-sensitive embedding for each word in a given sentence, which will be fed into downstream tasks. Compared to ELMo, BERT is deeper and contains much more parameters, thus possessing greater representation power. More im- portantly, rather than simply providing word em- beddings as features, BERT can be incorporated into a downstream task and gets ﬁne-tuned as an integrated task-speciﬁc architecture. BERT has, in general, been found to be supe- rior to ELMo and far superior to non-contextual embeddings on a variety of tasks, including those in the clinical domain (Si , 2019). For this reason, we only examine BERT here, rather than including ELMo or non-contextual embed- ding methods. Contextual Clinical & Biomedical Embeddings Several works have explored the utility of con- textual models in the clinical and biomedical do- mains. BioBERT (Lee , 2019) trains a BERT model over a corpus of biomedical research arti- cles sourced from PubMed3 article abstracts and PubMed Central4 article full texts. They ﬁnd the speciﬁcity offered by biomedical texts trans- lated to improved performance on several biomed- ical NLP tasks, and fully release their pre-trained BERT model. On clinical text, (Khin , 2018) uses a general-domain pretrained ELMo model towards the task of clinical text de-identiﬁcation, report- ing near state-of-the-art performance on the i2b2 2014 task (Stubbs and Uzuner, 2015; Stubbs , 2015) and state of the art performance on several axes of the HIPAA PHI dataset. Two works that we know of train contextual em- bedding models on clinical corpora. (Zhu , 2018) trains an ELMo model over a corpus of mixed clinical discharge summaries, clinical radiology notes and medically oriented wikipedia articles, then demonstrates improved performance on the i2b2 2010 task (Uzuner , 3https://www.ncbi.nlm.nih.gov/pubmed/ 4https://www.ncbi.nlm.nih.gov/pmc/ 2011). They release a pre-trained ELMo model along with their work, enabling further clinical NLP research to work with these powerful con- textual embeddings. (Si , 2019), released in late February 2019, train a clinical note corpus BERT language model and uses complex task-speciﬁc models to yield im- provements over both traditional embeddings and ELMo embeddings on the i2b2 2010 and 2012 tasks (Sun , 2013b,a) and the SemEval 2014 task 7 (Pradhan , 2014) and 2015 task 14 (El- hadad ) tasks, establishing new state-of-the- art results on all four corpora. However, this work neither releases their embeddings for the larger community nor examines the performance oppor- tunities offered by ﬁne-tuning BioBERT with clin- ical text or by training note-type speciﬁc embed- ding models, as we do. 3 Methods In this section, we ﬁrst describe our clinical text dataset, the details of the BERT training proce- dure, and ﬁnally the speciﬁc tasks we examine. 3.1 Data We use clinical text from the approximately 2 mil- lion notes in the MIMIC-III v1.4 database (John- son , 2016). Details of our text pre-processing procedure can be found in Appendix A. Note that while some of our tasks use a small subset of MIMIC notes in their corpora, we do not try to ﬁl- ter these notes out of our BERT pre-training proce- dure. We expect the bias this induces is negligible given the relative sizes of the two corpora. We train two varieties of BERT on MIMIC notes: Clinical BERT, which uses text from all note types, and Discharge Summary BERT, which uses only discharge summaries in an effort to tai- lor the corpus to downstream tasks (which often largely use discharge summaries). Note that we train our clinical BERT instantia- tions on all notes of the appropriate type(s), with- out regard for whether or not any individual note appeared in any of the train/test sets for the vari- ous tasks we use (two of which use a small subset of MIMIC notes either partially or completely as their backing corpora). We feel this has a negligi- ble impact given the dramatically larger size of the entire MIMIC corpus relative to the various task corpora. 3.2 BERT Training In this work, we aim to provide the pre-trained embeddings as a community resource, rather than demonstrate technical novelty in the training pro- cedure, and accordingly our BERT training pro- cedure is completely standard. As such, we have relegated speciﬁcs of the training procedure to Ap- pendix B. We trained two BERT models on clinical text: 1) Clinical BERT, initialized from BERT- Base, and 2) Clinical BioBERT, initialized from BioBERT. For all downstream tasks, BERT mod- els were allowed to be ﬁne-tuned, then the out- put BERT embedding was passed through a single linear layer for classiﬁcation, either at a per-token level for NER or de-ID tasks or applied to the sen- tinel “begin sentence” token for MedNLI. Note that this is a substantially lower capacity model than, for example, the Bi-LSTM layer used in (Si , 2019). This reduced capacity potentially limits performance on downstream tasks, but is in line with our goal of demonstrating the efﬁcacy of clinical-speciﬁc embeddings and releasing a pre- trained BERT model for these embeddings. We did not experiment with more complex representa- tions as our goal is not to necessarily surpass state- of-the-art performances on these tasks. Computational Cost Pre-processing and train- ing BERT on MIMIC notes took signiﬁcant com- putational resources. We estimate that our en- tire embedding model procedure took roughly 17 - 18 days of computational runtime using a single GeForce GTX TITAN X 12 GB GPU (and signiﬁ- cant CPU power and memory for pre-processing tasks). This is not including any time required to download or setup MIMIC or to train any ﬁ- nal downstream tasks. 18 days of continuous run- time is a signiﬁcant investment and may be be- yond the reach of some labs or institutions. This is precisely why we believe that releasing our pre- trained model will be useful to the community. 3.3 Tasks The Clinical BERT and Clinical BioBERT mod- els were applied to the MedNLI natural lan- guage inference task (Romanov and Shivade, 2018) and four i2b2 named entity recognition (NER) tasks, all in IOB format (Ramshaw and Marcus, 1995): i2b2 2006 1B de-identiﬁcation (Uzuner , 2007), i2b2 2010 concept extrac- tion (Uzuner , 2011), i2b2 2012 entity extrac- Dataset Metric Dim # Sentences Train Dev Test MedNLI Accuracy 3 11232 1395 1422 i2b2 2006 Exact F1 17 44392 5547 18095 i2b2 2010 Exact F1 7 14504 1809 27624 i2b2 2012 Exact F1 13 6624 820 5664 i2b2 2014 Exact F1 43 45232 5648 32586 Task dataset evaluation metrics, output dimen- sionality, and train/dev/test dataset sizes (in number of sentences). Exact F1 requires that the text span and la- bel be an exact match to be considered correct. tion challenge (Sun , 2013a,b), i2b2 2014 7A de-identiﬁcation challenge (Stubbs and Uzuner, 2015; Stubbs , 2015). Details of the IOB for- mat can be seen in the appendix, section C. All task dataset sizes, evaluation metrics, and number of classes are shown in Note that our two de-identiﬁcation (de-ID) datasets present synthetically-masked PHI in their texts—e.g., they replace instances of real names, hospitals, etc., with synthetic, but consistent and realistic, names, hospitals, etc. As a result, they present signiﬁcantly different text distribu- tions than traditionally de-identiﬁed text (such as MIMIC notes) which will instead present sentinel “PHI” symbols at locations where PHI was re- moved. 4 Results & Discussions In this section, we will ﬁrst describe quantitative comparisons of the various BERT models on the clinical NLP tasks we considered, and second de- scribe qualitative evaluations of the differences be- tween Clinical- and Bio- BERT. Clinical NLP Tasks Full results are shown in On three of the ﬁve tasks (MedNLI, i2b2 2010, and i2b2 2012), clinically ﬁne-tuned BioBERT shows improvements over BioBERT or general BERT. Notably, on MedNLI, clinical BERT actually yields a new state of the art, yield- ing a performance of 82.7% accuracy as compared to the prior state of the art of 73.5% (Romanov and Shivade, 2018) obtained via the InferSent model (Conneau , 2017). However, on our two de-ID tasks, i2b2 2006 and i2b2 2014, clinical BERT offers no improvements over Bio- or gen- eral BERT. This is actually not surprising, and is instead, we argue, a direct consequence of the na- Model MedNLI i2b2 2006 i2b2 2010 i2b2 2012 i2b2 2014 BERT 77.6% 93.9 83.5 75.9 92.8 BioBERT 80.8% 94.8 86.5 78.9 93.0 Clinical BERT 80.8% 91.5 86.4 78.5 92.6 Discharge Summary BERT 80.6% 91.9 86.4 78.4 92.8 Bio+Clinical BERT 82.7% 94.7 87.2 78.9 92.5 Bio+Discharge Summary BERT 82.7% 94.8 87.8 78.9 92.7 Accuracy (MedNLI) and Exact F1 score (i2b2) across various clinical NLP tasks. Model Disease Operations Generic Glucose Seizure Pneumonia Transfer Admitted Discharge Beach Newspaper Table BioBERT insulin episode vaccine drainage admission admission coast news tables exhaustion appetite infection division sinking wave rock ofﬁcial row dioxide attack plague transplant hospital sight reef industry dinner Clinical potassium headache consolidation transferred admission disposition shore publication scenario sodium stroke tuberculosis admitted transferred transfer ocean organization compilation sugar agitation infection arrival admit transferred land publicity technology Nearest neighbors for 3 sentinel words for each of 3 categories. In the Disease and operations categories, clinical BERT appears to show greater cohesion within the clinical domain than BioBERT, whereas for generic words, the methods do not differ much, as expected. ture of de-ID challenges. De-ID challenge data presents a different data distribution than MIMIC text. In MIMIC, PHI is identiﬁed and replaced with sentinel PHI mark- ers, whereas in the de-ID task, PHI is masked with synthetic, but realistic PHI. This data drift would be problematic for any embedding model, but will be especially damaging to contextual embedding models like BERT because the underlying sen- tence structure will have changed: in raw MIMIC, sentences with PHI will universally have a sentinel PHI token. In contrast, in the de-ID corpus, all such sentences will have different synthetic masks, meaning that a canonical, nearly constant sentence structure present during BERT’s training will be non-existent at task-time. For these reasons, we think it is sensible that clinical BERT is not suc- cessful on the de-ID corpora. Furthermore, this is a good example for the community given how prevalent the assumption is that contextual embed- ding models trained on task-like corpora will offer dramatic improvements. Overall, we feel our results demonstrates the utility of using domain-speciﬁc contextual embed- dings for non de-ID clinical NLP tasks. Addition- ally, on one task Discharge Summary BERT offers performance improvements over Clinical BERT, so it may be that adding greater speciﬁcity to the underlying corpus is helpful in some cases. We release both models with this work for public use. Qualitative Embedding Comparisons shows the nearest neighbors for 3 words each from 3 categories under BioBERT and Clinical BERT. These lists suggest that Clinical BERT re- tains greater cohesion around medical or clinic- operations relevant terms than does BioBERT. For example, the word “Discharge” is most closely associated with “admission,” “wave,” and “sight” under BioBERT, yet only the former seems rele- vant to clinical operations. In contrast, under Clin- ical BERT, the associated words all are meaningful in a clinical operations context. Limitations & Future Work This work has several notable limitations. First, we do not ex- periment with any more advanced model architec- tures atop our embeddings. This likely hurts our performance. Second, MIMIC only contains notes from the intensive care unit of a single healthcare institution (BIDMC). Differences in care practices across institutions are signiﬁcant, and using notes from multiple institutions could offer signiﬁcant gains. Lastly, our model shows no improvements for either de-ID task we explored. If our hypoth- esis is correct as to its cause, a possible solution could entail introducing synthetic de-ID into the source clinical text and using that as the source for de-ID tasks going forward. 5 Conclusion In this work, we pretrain and release clinically ori- ented BERT models, some trained solely on clini- cal text, and others ﬁne-tuned atop BioBERT. We ﬁnd robust evidence that our clinical embeddings are superior to general domain or BioBERT spe- ciﬁc embeddings for non de-ID tasks, and that us- ing note-type speciﬁc corpora can induce further selective performance beneﬁts. To the best of our knowledge, our work is the ﬁrst to release clini- cally trained BERT models. Our hope is that all clinical NLP researchers will be able to beneﬁt from these embeddings without the necessity of the signiﬁcant computational resources required to train these models over the MIMIC corpus. 6