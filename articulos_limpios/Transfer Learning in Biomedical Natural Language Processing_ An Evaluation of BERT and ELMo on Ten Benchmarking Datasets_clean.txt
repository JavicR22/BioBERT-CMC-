Proceedings of the BioNLP 2019 workshop, pages 58–65 Florence, Italy, August 1, 2019. c⃝2019 Association for Computational Linguistics 58 Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets Yifan Peng Shankai Yan Zhiyong Lu National Center for Biotechnology Information National Library of Medicine, National Institutes of Health Bethesda, MD, USA {yifan.peng, shankai.yan, zhiyong.lu}@nih.gov Abstract Inspired by the success of the General Lan- guage Understanding Evaluation benchmark, we introduce the Biomedical Language Un- derstanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain. The benchmark consists of ﬁve tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difﬁculties. We also evaluate several baselines based on BERT and ELMo and ﬁnd that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results. We make the datasets, pre-trained models, and codes pub- licly available at https://github.com/ ncbi-nlp/BLUE_Benchmark. 1 Introduction With the growing amount of biomedical informa- tion available in textual form, there have been signiﬁcant advances in the development of pre- training language representations that can be ap- plied to a range of different tasks in the biomedi- cal domain, such as pre-trained word embeddings, sentence embeddings, and contextual representa- tions (Chiu , 2016; Chen , 2019; Peters , 2017; Lee , 2019; Smalheiser , 2019). In the general domain, we have recently ob- served that the General Language Understand- ing Evaluation (GLUE) benchmark (Wang , 2018a) has been successfully promoting the de- velopment of language representations of general purpose (Peters , 2017; Radford , 2018; Devlin , 2019). To the best of our knowledge, however, there is no publicly available bench- marking in the biomedicine domain. To facilitate research on language representa- tions in the biomedicine domain, we present the Biomedical Language Understanding Evaluation (BLUE) benchmark, which consists of ﬁve dif- ferent biomedicine text-mining tasks with ten cor- pora. Here, we rely on preexisting datasets be- cause they have been widely used by the BioNLP community as shared tasks (Huang and Lu, 2015). These tasks cover a diverse range of text genres (biomedical literature and clinical notes), dataset sizes, and degrees of difﬁculty and, more impor- tantly, highlight common biomedicine text-mining challenges. We expect that the models that per- form better on all or most tasks in BLUE will ad- dress other biomedicine tasks more robustly. To better understand the challenge posed by BLUE, we conduct experiments with two base- lines: One makes use of the BERT model (Devlin , 2019) and one makes use of ELMo (Peters , 2017). Both are state-of-the-art language representation models and demonstrate promising results in NLP tasks of general purpose. We ﬁnd that the BERT model pre-trained on PubMed ab- stracts (Fiorini , 2018) and MIMIC-III clini- cal notes (Johnson , 2016) achieves the best results, and is signiﬁcantly superior to other mod- els in the clinical domain. This demonstrates the importance of pre-training among different text genres. In summary, we offer: (i) ﬁve tasks with ten biomedical and clinical text-mining corpora with different sizes and levels of difﬁculty, (ii) codes for data construction and model evaluation for fair comparisons, (iii) pretrained BERT models on PubMed abstracts and MIMIC-III, and (iv) base- line results. 2 Related work There is a long history of using shared lan- guage representations to capture text semantics in biomedical text and data mining research. Such re- 59 search utilizes a technique, termed transfer learn- ing, whereby the language representations are pre- trained on large corpora and ﬁne-tuned in a variety of downstream tasks, such as named entity recog- nition and relation extraction. One established trend is a form of word embed- dings that represent the semantic, using high di- mensional vectors (Chiu , 2016; Wang , 2018c; Zhang , 2019). Similar methods also have been derived to improve embeddings of word sequences by introducing sentence embed- dings (Chen , 2019). They always, however, require complicated neural networks to be effec- tively used in downstream applications. Another popular trend, especially in recent years, is the context-dependent representation. Different from word embeddings, it allows the meaning of a word to change according to the con- text in which it is used (Melamud , 2016; Pe- ters , 2017; Devlin , 2019; Dai , 2019). In the scientiﬁc domain, Beltagy re- leased SciBERT which is trained on scientiﬁc text. In the biomedical domain, BioBERT (Lee , 2019) and BioELMo (Jin , 2019) were pre- trained and applied to several speciﬁc tasks. In the clinical domain, Alsentzer released a clinical BERT base model trained on the MIMIC- III database. Most of these works, however, were evaluated on either different datasets or the same dataset with slightly different sizes of examples. This makes it challenging to fairly compare vari- ous language models. Based on these reasons, a standard benchmark- ing is urgently required. Parallel to our work, Lee introduced three tasks: named en- tity recognition, relation extraction, and QA, while Jin introduced NLI in addition to named entity recognition. To this end, we deem that BLUE is different in three ways. First, BLUE is selected to cover a diverse range of text genres, including both biomedical and clinical domains. Second, BLUE goes beyond sentence or sentence pairs by including document classiﬁcation tasks. Third, BLUE provides a comprehensive suite of codes to reconstruct dataset from scratch without removing any instances. 3 Tasks BLUE contains ﬁve tasks with ten corpora that cover a broad range of data quantities and difﬁ- culties ( Here, we rely on preexisting datasets because they have been widely used by the BioNLP community as shared tasks. 3.1 Sentence similarity The sentence similarity task is to predict simi- larity scores based on sentence pairs. Following common practice, we evaluate similarity by using Pearson correlation coefﬁcients. BIOSSES is a corpus of sentence pairs selected from the Biomedical Summarization Track Training Dataset in the biomedical do- main (So˘gancıo˘glu , 2017).1 To develop BIOSSES, ﬁve curators judged their similarity, us- ing scores that ranged from 0 (no relation) to 4 (equivalent). Here, we randomly select 80% for training and 20% for testing because there is no standard splits in the released data. MedSTS is a corpus of sentence pairs se- lected from Mayo Clinic’s clinical data ware- house (Wang , 2018b). To develop MedSTS, two medical experts graded the sentence’s seman- tic similarity scores from 0 to 5 (low to high sim- ilarity). We use the standard training and testing sets in the shared task. 3.2 Named entity recognition The aim of the named entity recognition task is to predict mention spans given in the text (Ju- rafsky and Martin, 2008). The results are evalu- ated through a comparison of the set of mention spans annotated within the document with the set of mention spans predicted by the model. We eval- uate the results by using the strict version of preci- sion, recall, and F1-score. For disjoint mentions, all spans also must be strictly correct. To construct the dataset, we used spaCy2 to split the text into a sequence of tokens when the original datasets do not provide such information. BC5CDR is a collection of 1,500 PubMed titles and abstracts selected from the CTD-Pﬁzer cor- pus and was used in the BioCreative V chemical- disease relation task (Li , 2016).3 The dis- eases and chemicals mentioned in the articles were annotated independently by two human experts with medical training and curation experience. We use the standard training and test set in the 1http://tabilab.cmpe.boun.edu.tr/ BIOSSES/ 2https://spacy.io/ 3https://biocreative.bioinformatics. udel.edu/tasks/biocreative-v/ track-3-cdr/ 60 Corpus Train Dev Test Task Metrics Domain Avg sent len MedSTS, sentence pairs 675 75 318 Sentence similarity Pearson Clinical 25.8 BIOSSES, sentence pairs 64 16 20 Sentence similarity Pearson Biomedical 22.9 BC5CDR-disease, mentions 4182 4244 4424 NER F1 Biomedical 22.3 BC5CDR-chemical, mentions 5203 5347 5385 NER F1 Biomedical 22.3 ShARe/CLEFE, mentions 4628 1075 5195 NER F1 Clinical 10.6 DDI, relations 2937 1004 979 Relation extraction micro F1 Biomedical 41.7 ChemProt, relations 4154 2416 3458 Relation extraction micro F1 Biomedical 34.3 i2b2 2010, relations 3110 11 6293 Relation extraction F1 Clinical 24.8 HoC, documents 1108 157 315 Document classiﬁcation F1 Biomedical 25.3 MedNLI, pairs 11232 1395 1422 Inference accuracy Clinical 11.9 BLUE tasks BC5CDR shared task (Wei , 2016). ShARe/CLEF eHealth Task 1 Corpus is a col- lection of 299 deidentiﬁed clinical free-text notes from the MIMIC II database (Suominen , 2013).4 The disorders mentioned in the clini- cal notes were annotated by two professionally trained annotators, followed by an adjudication step, resulting in high inter-annotator agreement. We use the standard training and test set in the ShARe/CLEF eHealth Tasks 1. 3.3 Relation extraction The aim of the relation extraction task is to pre- dict relations and their types between the two enti- ties mentioned in the sentences. The relations with types were compared to annotated data. We use the standard micro-average precision, recall, and F1-score metrics. DDI extraction 2013 corpus is a collection of 792 texts selected from the DrugBank database and other 233 Medline abstracts (Herrero-Zazo , 2013).5 The drug-drug interactions, includ- ing both pharmacokinetic and pharmacodynamic interactions, were annotated by two expert phar- macists with a substantial background in pharma- covigilance. In our benchmark, we use 624 train ﬁles and 191 test ﬁles to evaluate the performance and report the micro-average F1-score of the four DDI types. ChemProt consists of 1,820 PubMed abstracts with chemical-protein interactions annotated by domain experts and was used in the BioCre- ative VI text mining chemical-protein interactions shared task (Krallinger , 2017).6 We use the 4https://physionet.org/works/ ShAReCLEFeHealth2013/ 5http://labda.inf.uc3m.es/ddicorpus 6https://biocreative. bioinformatics.udel.edu/news/corpora/ standard training and test sets in the ChemProt shared task and evaluate the same ﬁve classes: CPR:3, CPR:4, CPR:5, CPR:6, and CPR:9. i2b2 2010 shared task collection consists of 170 documents for training and 256 documents for testing, which is the subset of the original dataset (Uzuner , 2011).7 The dataset was collected from three different hospitals and was annotated by medical practitioners for eight types of relations between problems and treatments. 3.4 Document multilabel classiﬁcation The multilabel classiﬁcation task predicts multiple labels from the texts. HoC (the Hallmarks of Cancers corpus) con- sists of 1,580 PubMed abstracts annotated with ten currently known hallmarks of cancer (Baker , 2016).8 Annotation was performed at sentence level by an expert with 15+ years of experience in cancer research. We use 315 (∼20%) abstracts for testing and the remaining abstracts for train- ing. For the HoC task, we followed the common practice and reported the example-based F1-score on the abstract level (Zhang and Zhou, 2014; Du , 2019). 3.5 Inference task The aim of the inference task is to predict whether the premise sentence entails or contradicts the hy- pothesis sentence. We use the standard overall ac- curacy to evaluate the performance. MedNLI is a collection of sentence pairs se- lected from MIMIC-III (Romanov and Shivade, 2018).9 Given a premise sentence and a hy- chemprot-corpus-biocreative-vi/ 7https://www.i2b2.org/NLP/DataSets/ 8https://www.cl.cam.ac.uk/˜sb895/HoC. html 9https://physionet.org/physiotools/ mimic-code/mednli/ 61 pothesis sentence, two board-certiﬁed radiologists graded whether the task predicted whether the premise entails the hypothesis (entailment), con- tradicts the hypothesis (contradiction), or neither (neutral). We use the same training, development, and test sets in Romanov and Shivade (Romanov and Shivade, 2018). 3.6 Total score Following the practice in Wang (2018a) and Lee , we use a macro-average of F1- scores and Pearson scores to determine a system’s position. 4 Baselines For baselines, we evaluate several pre-training models as described below. The original code for the baselines is available at https://github. com/ncbi-nlp/NCBI_BERT. 4.1 BERT 4.1.1 Pre-training BERT BERT (Devlin , 2019) is a contextualized word representation model that is pre-trained based on a masked language model, using bidirec- tional Transformers (Vaswani , 2017). In this paper, we pre-trained our own model BERT on PubMed abstracts and clinical notes (MIMIC-III). The statistics of the text corpora on which BERT was pre-trained are shown in Corpus Words Domain PubMed abstract > 4,000M Biomedical MIMIC-III > 500M Clinical Corpora We initialized BERT with pre-trained BERT provided by (Devlin , 2019). We then con- tinue to pre-train the model, using the listed cor- pora. We released our BERT-Base and BERT-Large models, using the same vocabulary, sequence length, and other conﬁgurations provided by De- vlin . Both models were trained with 5M steps on the PubMed corpus and 0.2M steps on the MIMIC-III corpus. 4.1.2 Fine-tuning with BERT BERT is applied to various downstream text- mining tasks while requiring only minimal archi- tecture modiﬁcation. For sentence similarity tasks, we packed the sentence pairs together into a single sequence, as suggested in Devlin . For named entity recognition, we used the BIO tags for each token in the sentence. We considered the tasks similar to machine translation, as predict- ing the sequence of BIO tags from the input sen- tence. We treated the relation extraction task as a sen- tence classiﬁcation by replacing two named en- tity mentions of interest in the sentence with pre- deﬁned tags (e.g., @GENE) (Lee , 2019). For example, we used “@CHEMI- CAL binding.” to replace the orig- inal sentence “Citalopram protected against the RTI-76-induced inhibition of SERT binding.” in which “citalopram” and “SERT” has a chemical- gene relation. For multi-label tasks, we ﬁne-tuned the model to predict multi-labels for each sentence in the document. We then combine the labels in one doc- ument and compare them with the gold-standard. Like BERT, we provided sources code for ﬁne- tuning, prediction, and evaluation to make it straightforward to follow those examples to use our BERT pre-trained models for all tasks. 4.2 Fine-tuning with ELMo We adopted the ELMo model pre-trained on PubMed abstracts (Peters , 2017) to accom- plish the BLUE tasks.10 The output of ELMo em- beddings of each token is used as input for the ﬁne-tuning model. We retrieved the output states of both layers in ELMo and concatenated them into one vector for each word. We used the maxi- mum sequence length 128 for padding. The learn- ing rate was set to 0.001 with an Adam optimizer. We iterated the training process for 20 epochs with batch size 64 and early stopped if the training loss did not decrease. For sentence similarity tasks, we used bag of embeddings with the average strategy to transform the sequence of word embeddings into a sentence embedding. Afterward, we concatenated two sen- tence embeddings and fed them into an architec- ture with one dense layer to predict the similarity of two sentences. 10https://allennlp.org/elmo 62 Task Metrics SOTA* ELMo BioBERT Our BERT Base Base Large Large (P) (P+M) (P) (P+M) MedSTS Pearson 83.6 68.6 84.5 84.5 84.8 84.6 83.2 BIOSSES Pearson 84.8 60.2 82.7 89.3 91.6 86.3 75.1 BC5CDR-disease F 84.1 83.9 85.9 86.6 85.4 82.9 83.8 BC5CDR-chemical F 93.3 91.5 93.0 93.5 92.4 91.7 91.1 ShARe/CLEFE F 70.0 75.6 72.8 75.4 77.1 72.7 74.4 DDI F 72.9 78.9 78.8 78.1 79.4 79.9 76.3 ChemProt F 64.1 66.6 71.3 72.5 69.2 74.4 65.1 i2b2 F 73.7 71.2 72.2 74.4 76.4 73.3 73.9 HoC F 81.5 80.0 82.9 85.3 83.1 87.3 85.3 MedNLI acc 73.5 71.4 80.5 82.2 84.0 81.5 83.8 Total 78.8 80.5 82.2 82.3 81.5 79.2 * SOTA, state-of-the-art as of April 2019, to the best of our knowledge: MedSTS, BIOSSES (Chen , 2019); BC5CDR-disease, BC5CDR-chem (Yoon , 2018); ShARe/CLEFE (Leaman , 2015); DDI (Zhang , 2018). Chem-Prot (Peng , 2018); i2b2 (Rink , 2011); HoC (Du , 2019); MedNLI (Romanov and Shivade, 2018). P: PubMed, P+M: PubMed + MIMIC-III Baseline performance on the BLUE task test sets. For named entity recognition, we used a Bi- LSTM-CRF implementation as a sequence tag- ger (Huang , 2015; Si , 2019; Lample , 2016). Speciﬁcally, we concatenated the GloVe word embeddings (Pennington , 2014), character embeddings, and ELMo embeddings of each token and fed the combined vectors into the sequence tagger to predict the label for each to- ken. The GloVe word embeddings11 and character embeddings have 100 and 25 dimensions, respec- tively. The hidden sizes of the Bi-LSTM are also set to 100 and 25 for the word and character em- beddings, respectively. For relation extraction and multi-label tasks, we followed the steps in ﬁne-tuning with BERT but used the averaged ELMo embeddings of all words in each sentence as the sentence embedding. 5 Benchmark results and discussion We pre-trained four BERT models: BERT-Base (P), BERT-Large (P), BERT-Base (P+M), BERT- Large (P+M) on PubMed abstracts only, and the combination of PubMed abstracts and clinical notes, respectively. We present performance on the main benchmark tasks in More de- tailed comparison is shown in the Appendix A. 11https://nlp.stanford.edu/projects/ glove/ Overall, our BERT-Base (P+M) that were pre- trained on both PubMed abstract and MIMIC-III achieved the best results across ﬁve tasks, even though it is only slightly better than the one pre- trained on PubMed abstracts only. Compared to the tasks in the clinical domain and biomedical do- main, BERT-Base (P+M) is signiﬁcantly superior to other models. This demonstrates the importance of pre-training among different text genres. When comparing BERT pre-trained using the base settings against that using the large settings, it is a bit surprising that BERT-Base is better than BERT-Large except in relation extraction and document classiﬁcation tasks. Further analysis shows that, on these tasks, the average length of sentences is longer than those of others (Ta- ble 1). In addition, BERT-Large pre-trained on PubMed and MIMIC is worse than other models overall. However, BERT-Large (P) performs the best in the multilabel task, even compared with the feature-based model utilizing enriched ontol- ogy (Yan and Wong, 2017). This is partially be- cause the MIMIC-III data are relatively smaller than the PubMed abstracts and, thus, cannot pre- train the large model sufﬁciently. In the sentence similarity tasks, BERT-Base (P+M) achieves the best results on both datasets. Because the BIOSSES dataset is very small (there 63 are only 16 sentence pairs in the test set), all BERT models’ performance was unstable. This prob- lem has also been noted in the work of Devlin when the model was evaluated on the GLUE benchmarking. Here, we obtained the best results by following the same strategy: selecting the best model on the development set after sev- eral runs. Other possible ways to overcome this issue include choosing the model with the best per- formance from multiple runs or averaging results from multiple ﬁne-tuned models. In the named entity recognition tasks, BERT- Base (P) achieved the best results on two biomedi- cal datasets, whereas BERT-Base (P+M) achieved the best results on the clinical dataset. In all cases, we observed that the winning model ob- tained higher recall than did the others. Given that we use the pre-deﬁned vocabulary in the original BERT and that this task relies heavily on the to- kenization, it is possible that using BERT as per- taining to a custom sentence piece tokenizer may further improve the model’s performance. 6 Conclusion In this study, we introduce BLUE, a collection of resources for evaluating and analyzing biomedical natural language representation models. We ﬁnd that the BERT models pre-trained on PubMed ab- stracts and clinical notes see better performance than do most state-of-the-art models. Detailed analysis shows that our benchmarking can be used to evaluate the capacity of the models to un- derstand the biomedicine text and, moreover, to shed light on the future directions for developing biomedicine language representations.