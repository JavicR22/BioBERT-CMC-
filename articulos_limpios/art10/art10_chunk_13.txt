##em. 15 411 BC4CHEMD ( Krallinger, 2015 ) Drug / Chem. 79 842 BC2GM ( Smith, 2008 ) Gene / Protein 20 703 JNLPBA ( Kim, 2004 ) Gene / Protein 35 460 LINNAEUS ( Gerner, 2010 ) Species 4077 Species - 800 ( Paﬁlis, 2013 ) Species 3708 Note : The number of annotations from Habibi and Zhu is provided. Statistics of the biomedical relation extraction datasets Dataset Entity type Number of relations GAD ( Bravo, 2015 ) Gene – disease 5330 EU - ADR ( Van Mulligen, 2012 ) Gene – disease 355 CHEMPROT ( Krallinger, 2017 ) Protein – chemical 10 031 Note : For the CHEMPROT dataset, the number of relations in the train - ing, validation and test sets was summed. Statistics of biomedical question answering datasets Dataset Number of train Number of test BioASQ 4b - factoid ( Tsatsaronis, 2015 ) 327 161 BioASQ 5b - factoid ( Tsatsaronis, 2015 ) 486 150 BioASQ 6b - factoid ( Tsatsaronis, 2015 ) 618 161 BioBERT 1237 Downloaded from https : / / academic. oup. com / bioinformatics / article / 36 / 4 / 1234 / 5566506 by guest on 30 October 2025 entities. While BERT often gives incorrect answers to simple bio - medical questions, BioBERT provides correct answers to such ques - tions. Also, BioBERT can provide longer named entities as answers. 6 Conclusion In this article, we introduced BioBERT, which is a pre - trained lan - guage representation model for biomedical text mining. We showed that pre - training BERT on biomedical corpora is crucial in applying it to the biomedical domain. Requiring minimal task - specific architectural modification, BioBERT outperforms previous models on biomedical text mining tasks such as NER, RE and QA. The pre - released version of BioBERT ( January 2019 ) has already been shown to be very effective in many biomedical text mining tasks such as NER for clinical notes ( Alsentzer, 2019 ),