on PubMed abstracts ( PubMed ) and PubMed Central full - text articles ( PMC ). The text corpora used for pre - training of BioBERT are listed in and the tested combinations of text corpora are listed in For computational efficiency, whenever the Wiki þ Books corpora were used for pre - training, we initialized BioBERT with the pre - trained BERT model provided by Devlin. We define BioBERT as a language representation model whose pre - training corpora includes biomedical corpora ( e. g. BioBERT ( þ PubMed ) ). For tokenization, BioBERT uses WordPiece tokenization ( Wu, 2016 ), which mitigates the out - of - vocabulary issue. With WordPiece tokenization, any new words can be represented by fre - quent subwords ( e. g. Immunoglobulin ¼ > I # # mm # # uno # # g # # lo # # bul # # in ). We found that using cased vocabulary ( not lower - casing ) results in slightly better performances in downstream tasks. Although we could have constructed new WordPiece vocabulary based on biomedical corpora, we used the original vocabulary of BERTBASE for the following reasons : ( i ) compatibility of BioBERT with BERT, which allows BERT pre - trained on general domain cor - pora to be re - used, and makes it easier to interchangeably use exist - ing models based on BERT and BioBERT and ( ii ) any new words may still be represented and fine - tuned for the biomedical domain using the original WordPiece vocabulary of BERT. 3. 3 Fine - tuning BioBERT With minimal architectural modification, BioBERT can be applied to various downstream text mining tasks. We fine - tune BioBERT on the following three representative biomedical text mining tasks : NER, RE and QA. Named entity recognition is one of the most fundamental bio - medical text mining tasks, which involves recognizing numerous do - main - specific proper nouns in a biomedical corpus. While most previous works were built upon different combinations of LSTMs and CRFs ( Giorgi and Bader, 2018 ; Habibi, 2017 ; Wang, 2018 ), BERT