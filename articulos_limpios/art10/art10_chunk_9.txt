not have separate test sets ( e. g. GAD, EU - ADR ). We compare BERT and BioBERT with the current state - of - the - art models and report their scores. Note that the state - of - the - art models each have a different architecture and training procedure. For instance, the state - of - the - art model by Yoon trained on the JNLPBA dataset is based on multiple Bi - LSTM CRF models with character level CNNs, while the state - of - the - art model by Giorgi and Bader trained on the LINNAEUS dataset uses a Bi - LSTM CRF model with character level LSTMs and is additional - ly trained on silver - standard datasets. On the other hand, BERT and List of text corpora used for BioBERT Corpus Number of words Domain English Wikipedia 2. 5B General BooksCorpus 0. 8B General PubMed Abstracts 4. 5B Biomedical PMC Full - text articles 13. 5B Biomedical Pre - training BioBERT on different combinations of the fol - lowing text corpora : English Wikipedia ( Wiki ), BooksCorpus ( Books ), PubMed abstracts ( PubMed ) and PMC full - text articles ( PMC ) Model Corpus combination BERT ( Devlin, 2019 ) Wiki þ Books BioBERT ( þPubMed ) Wiki þ Books þ PubMed BioBERT ( þPMC ) Wiki þ Books þ PMC BioBERT ( þPubMed þ PMC ) Wiki þ Books þ PubMed þ PMC 1236 J. Lee Downloaded from https : / / academic. oup. com / bioinformatics / article / 36 / 4 / 1234 / 5566506 by guest on 30 October 2025 BioBERT have exactly the same structure, and use only the gold standard datasets and not any additional datasets. 4. 2 Experimental setups We used the BERTBASE model pre - trained on English Wikipedia and BooksCorpus for 1M steps. BioBERT v1. 0 ( þ PubMed þ PMC ) is the version of BioBERT ( þ PubMed þ PMC ) trained for 470 K steps. When