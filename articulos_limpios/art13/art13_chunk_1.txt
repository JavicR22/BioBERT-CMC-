npj | digital medicine Article Published in partnership with Seoul National University Bundang Hospital https : / / doi. org / 10. 1038 / s41746 - 025 - 01533 - 1 Medical foundation largelanguage models for comprehensive text analysis and beyond Check for updates Qianqian Xie1, 5, Qingyu Chen1, 5, Aokun Chen2, 5, Cheng Peng2, Yan Hu3, Fongci Lin1, Xueqing Peng1, Jimin Huang1, Jeffrey Zhang1, Vipina Keloth1, Xinyu Zhou1, Lingfei Qian1, Huan He1, Dennis Shung1, 4, Lucila Ohno - Machado1, Yonghui Wu2, Hua Xu1 & Jiang Bian2 Recent advancements in large language models ( LLMs ) show signiﬁcant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me - LLaMA, a family of open - source medical LLMs integrating extensive domain - speciﬁc knowledge with robust instruction - following capabilities. Me - LLaMA is developed through continual pretraining and instruction tuning of LLaMA2 models using diverse biomedical and clinical data sources ( e. g., biomedical literature and clinical notes ). We evaluated Me - LLaMA on six text analysis tasks using 12 benchmarks ( e. g., PubMedQA and MIMIC - CXR ) and assessed its clinical utility in complex case diagnosis through automatic and human evaluations. Me - LLaMA outperforms existing open medical LLMs in zero - shot and supervised settings and surpasses ChatGPT and GPT - 4 after task - speciﬁc instruction tuning for most text analysis tasks. Its performance is also comparable to ChatGPT and GPT - 4 for diagnosing complex clinical cases. Our ﬁndings highlight the importance of combining domain - speciﬁc continual pretraining with instruction tuning to enhance performance in medical LLMs. Large language models ( LLMs ) have shown great potential in improving medical applications such as clinical documentation, diagnostic accuracy, andpatientcaremanagement1 – 3. However, general - domainLLMsoftenlack specialized medical knowledge because they are primarily trained on non - medical datasets4, limiting their effectiveness in healthcare settings.