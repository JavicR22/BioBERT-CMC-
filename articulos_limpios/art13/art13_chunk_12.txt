plays a vital role in mitigating the knowledge - forgetting issue during pre - training2, 9. However, determining the optimal balance between general domain data and spe - cialized medical data is nontrivial, requiring careful empirical analysis. Future studies should examine methods to better determine the opti - mal ratio. The cost - effectiveness of instruction tuning is another important consideration. Pre - training, exempliﬁed by the LLaMA2 70B model, is notably resource - heavy, requiring about 160 * 700 GPU hours per epoch. Conversely, instruction tuning is far less resource - demanding, needing roughly 8 * 70 GPU hours per epoch, making it much more affordable than pre - training. While continual pre - training aims to incorporate specialized medical knowledge into the model, the observed performance improve - ments, particularly for smaller models like Me - LLaMA 13B, are relatively modest. This limited improvement can be attributed to several factors. First, theamountofdomain - speciﬁcpre - trainingdatausedissigniﬁcantlysmaller compared to the original pre - training data of LLaMA2, which exceeds 2 T tokens. This discrepancy suggests that larger amounts of domain - speciﬁc data may be required to fully activate the model ’ s potential. Second, the continual pre - training strategy itself could be optimized further. As noted earlier, the process faces challenges such as catastrophic forgetting, where themodellosesgeneral - domainknowledgeduringadaptationtospecialized data. Despite this, models trained only with the instruction tuning demonstrate that instruction tuning alone can signiﬁcantly enhance per - formance at a fraction of the computational cost. This highlights instruction tuning as a practical and cost - effective alternative, particularly in scenarios where computational resources are limited. The Me - LLaMA models, available in both 13B and 70B sizes, as well as in base and chat - optimized versions, enable a wide array of medical appli - cations, guided by the crucial balance between model size and resource availability. The base models provide a strong foundation for supervised ﬁne - tuning on specialized tasks, while the chat - optimized versions excel in instruction - following and zero - shot scenarios. Larger models, like the 70B, deliver superior reasoning capabilities but require signiﬁcant computational resources,