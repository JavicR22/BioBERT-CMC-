RE, Classiﬁcation, Summarization, NLI, Medical Diagnosis 06 / 05 / 2024 https : / / doi. org / 10. 1038 / s41746 - 025 - 01533 - 1 Article npj Digital Medicine | 8 : 141 6 real - world clinical scenarios and reasoning, we included de - identiﬁed free - text clinical notes from MIMIC - III20, MIMIC - IV21, and MIMIC - CXR22. MIMIC - III contains 112, 000 clinical reports records. MIMIC - IV contains 331, 794 de - identiﬁed discharge summariesand 2, 321, 355radiologyreports. MIMIC - CXR adds further depth with 227, 835 radiology reports for radiographic studies. Moreover, to prevent the model from forgetting acquired general knowledge, we incorporated a subset from the RedPajama23 dataset, a replication of LLaMA2 ’ s pre - training data. This dataset is composed of diverse data slices, including processed Common - Crawl dumps, GitHub data, scientiﬁc articles from arXiv, a subset of Wikipedia pages, and popular websites from StackExchange. Our dataset was structured with a 15 : 1 : 4 ratio of biomedical, clinical, to general domain data and contains a total of 129 billion tokens, making it the largest pre - training dataset in the medical domain currently available. Medical instruction tuning data To enhance our model ’ s ability to follow instructions and generalize across diverse medical tasks, we further developed a novel medical instruction tuning dataset with 214, 595 high - quality samples from a wide array of data sources. This dataset stands out from those used in existing medical LLMs due to its comprehensive coverage of both biomedical and clinical domains. Our data sources included biomedical literature, clinical notes, clinical guidelines, wikidoc, knowledge graphs, and general domain data, as shown in The diverse tasks aim to reﬁne the model ’ s ability to process and respond to medical information accurately and contextually. Detailed prompts for each data and the data example are shown in the Supple - mentary Information, Supplementary Training details As shown in we developed the Me - LLaMA 13B and 70B base models by continually pre - training