the LLaMA2 13B and 70B models. These base models were then instruction - tuned to create the Me - LLaMA - 13B - chat and Me - LLaMA - 70B - chat models. The ﬁrst phase aims to develop Me - LLaMA base models, and adapt LLaMA2 models to better understand and generate text relevant to the medical context using the pre - training datasets we constructed. The objective is to enhance the model ’ s ability to understand and generate domain - speciﬁc text by optimizing it to predict the next word in a sequence based on the preceding context. This training was executed on the University of Florida ’ s HiPerGator AI supercomputer with 160 A100 80GB GPUs. We employed the AdamW optimizer with hyperparameters set to β1 to 0. 9 and β2 to 0. 95, alongside a weight decay of 0. 00001 and a learning rate of 8e - 6. We used a cosine learning rate scheduler with a 0. 05 warmup ratio for gradual adaptation to training complexity and bf16 precision for com - putational efﬁciency. Gradient accumulation was set to 16 steps, and training was limited to one epoch. We utilized DeepSpeed24 for model parallelism. We further ﬁne - tuned Me - LLaMA base models to develop Me - LLaMA chat models, using the developed 214k instruction samples. In this phase, the models are trained to produce accurate and contextually appropriate responses to speciﬁc input instructions. Executed using 8 A100 GPUs, the ﬁne - tuning process was set to run for 3 epochs with a learning rate of 1e - 5. We used a weight decay of 0. 00001 and a warmup ratio of 0. 01 for regularization and gradual learning rate increase. We utilized LoRA - based25 parameter - efﬁcient ﬁne - tuning. Evaluation benchmark Existing studies2, 3, 9 in the medical domain have primarily focused on eval - uating the QA task. In this study, we build an extensive medical evaluation benchmark ( MIBE ), encompassing six critical text analysis tasks : QA, NER, RE, Text Classiﬁcation, Text Summarization and NLI. These tasks collec - tively