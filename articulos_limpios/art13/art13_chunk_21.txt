Meditron7B / 70B9 : these are medical LLMs based on LLaMA2 - 7B / 70B, continually pre - trained with a mix of clinical guidelines, medical papers and abstracts. Zero - shot Learning We assessed our Me - LLaMA 13 / 70B - chat models ’ zero - shot learning cap - abilities, which are key for new task understanding and response without speciﬁc prior training. We compared our models and baseline models ’ zero - shot, using standardized prompts ( detailed in the Supplementary Infor - mation, Supplementary for each test dataset from We compared Me - LLaMA 13 / 70B - chat models with the following baseline models : ChatGPT / GPT - 44 : SOTA commercialized LLMs. We used the version of “ gpt - 3. 5 - turbo - 0301 ” for ChatGPT, and the version of “ gpt - 4 - 0314 ” for GPT - 4. LLaMA2 - 7B / 13B / 70B - chat6 models were adaptations of the LLaMA2 series, optimized for dialogue and conversational scenarios. Medalpaca - 7B / 13B3 models were based on LLaMA - 7B / 13B, speciﬁcally ﬁne - tuned for tasks in the medical domain. The PMC - LLaMA - 13B - chat2 model is an instruction - tuned medical LLM based on PMC - LLaMA - 13B. The AlpaCare - 13B11 model is speciﬁcally tailored for clinical tasks based on LLaMA - 2 13B by instruction tuning. Meditron 70B9 is a medical LLM, continually pre - trained with a mix of clinical guidelines, biomedical papers, and abstracts based on LLaMA2 70B. Data availability All datasets employed in the continual pre - training process and evaluation are accessible from their original published venues. The PubMed Central and PubMed Abstracts subset from The Pile are available at https : / / huggingface. co / datasets / EleutherAI / pile. MIMIC - IV and MIMIC - CXR datasets can be accessed under the PhysioNet Credentialed Health Data Use Agreement 1. 5. 0 at https :