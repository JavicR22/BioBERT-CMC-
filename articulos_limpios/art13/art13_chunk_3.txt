Biomedical Informatics, College of Medicine, University of Florida, Gainesville, FL, USA. 3School of Biomedical Informatics, University of Texas Health Science, Center at Houston, Houston, TX, USA. 4Department of Medicine ( Digestive Diseases ), Yale School of Medicine, Yale University, New Haven, CT, USA. 5These authors contributed equally : Qianqian Xie, Qingyu Chen, Aokun Chen. e - mail : hua. xu @ yale. edu ; bianji @ iu. edu npj Digital Medicine | 8 : 141 1 1234567890 ( ) :, ; 1234567890 ( ) :, ; clinical notes, we generated the largest biomedical pre - training dataset ( 129B tokens ), compared to the previous efforts ( i. e., 79B tokens in PMC - LLaMA as the highest ). Evaluations have predominantly centered on medical question - answering ( QA ) tasks, lacking comprehensive assess - ments on the generalizability of those foundation models across diverse medical tasks. To overcome these limitations, we present Me - LLaMA, a novel family of open - source medical large language models that uniquely integrate extensive domain - speciﬁc knowledge with robust instruction - following capabilities. Me - LLaMA comprises foundation models ( Me - LLaMA 13B and 70B ) and their chat - enhanced versions, developed through compre - hensive continual pretraining and instruction tuning of LLaMA26 models. Leveraging an extensive medical dataset — combining 129 billion pretrain - ingtokensand214, 000instructionsamplesfromscientiﬁcliterature, clinical guidelines, and electronic health record clinical notes — Me - LLaMA excels across a wide spectrum of medical text analysis and real - world clinical tasks. Prior studies2, 3, 7 – 12 have primarily focused on evaluating the QA task. For example, PMC - LLaMA2 and Meditron9 evaluated their model performance on medical QA tasks derived from domain - speciﬁc literature, while MedAlpaca3 and ChatDoctor10 focused on conversational QA. In contrast, we conduct a comprehensive evaluation covering six critical tasks — question answering, relation extraction, named entity