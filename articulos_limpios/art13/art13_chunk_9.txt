in the ﬁeld. Performance of complex clinical case diagnosis shows the top - K ( 1 K 5 ) accuracy of Me - LLaMA - 70B - chat, ChatGPT, GPT - 4, and LLaMA2 - 70B - chat, in the complex clinical case diagnosis task. We can see Me - LLaMA - 70B - chat model achieved com - parable performance with GPT - 4 and ChatGPT and signiﬁcantly outper - forms LLaMA2 - 70B - chat. The human evaluation result in again shows that Me - LLaMA - 70B - chat outperformed GPT - 4 in both top - 1 and top - 5 accuracy. These results demonstrated the potential of Me - LLaMA models for challenging clinical applications. Impact of continual pretraining and instruction tuning demonstrates the impact of continual pre - training and instruction tuning on zero - shot performance across medical NLP tasks. It clearly demonstrates that both continual pre - training and instruction tuning sig - niﬁcantly enhanced the zero - shot capabilities of models. Instruction tuning aloneprovidessigniﬁcantperformanceimprovementsoverthebaseLLaMA2 models, as seen in LLaMA2 13B, where accuracy on PubMedQA increases from0. 216 to 0. 436. This suggests that instruction tuning is highly effective in enhancing the model ’ s ability to follow task - speciﬁc prompts. In contrast, continual pre - training on medical data yields relatively modest improve - ments, particularly for smaller models. Me - LLaMA 13B shows only slight gains over LLaMA2 13B, likely due to the smaller scale of domain - speciﬁc pre - training data compared to LLaMA2 ’ s original training corpus, which exceeds 2 T tokens. Additionally, continual pre - training may not provide as strong of a task - speciﬁc signal as instruction tuning, limiting its impact in zero - shot settings. However, for larger models like Me - LLaMA 70B, con - tinual pre - training results in more notable improvements, with performance gains ranging from 2. 1 % to 55 % across various datasets, demonstrating its value in capturing specialized domain knowledge. The best results are con - sistently achieved when both continual pre - training and instruction tuning are