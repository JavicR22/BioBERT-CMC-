of biomedical literature, clinical notes, and general domain data. Our dataset integrates a vast collection of biomedical literature from PubMed Central and PubMed Abstracts, sourced from the Pile dataset19. The PubMed Central subset includes 3, 098, 931 biomedical articles, and the PubMed Abstracts section encompasses abstracts from 15, 518, 009 documents. This comprehensive biomedical dataset provides a rich source of medical knowledge and research ﬁndings. To incorporate Overview of the study. Our study has three main components including pre - training, instruc - tion ﬁne - tuning and evaluation. Pre - training : we ﬁrst developed the Me - LLaMA base models by continual pre - training LLaMA2 with 129 billion tokens from mixed pre - training text data. Instruc - tion ﬁne - tuning : Me - LLaMA - chat models were further developed by instruction - tuning Me - LLaMA base models with 214 K instructions. Eva - luation : Finally, we evaluated the Me - LLaMA base models in a supervised learning setting across six text analysis tasks, and the Me - LLaMA - chat models in a zero - shot setting on both text analysis tasks and a clinical diagnosis task. The comparison of Me - LLaMA models and existing open source medical LLMs Model Backbone Model size Biomedical literature Clinical notes Continual pre - training ( # of tokens ) Instruction tuning ( # of instructions ) Evaluation tasks Release date MedAlpaca3 LLaMA 7 / 13B - 160 K QA 04 / 14 / 2023 ChatDoctor12 LLaMA2 7B - 100 K QA 05 / 24 / 2023 AlpaCare28 LLaMA 7 / 13B - 52 K QA, Summarization 10 / 23 / 2023 Clinical LLaMA11 LLaMA 7B - - Classiﬁcation 07 / 06 / 2023 Meditron10 LLaMA2 7 / 70B 48B - QA 11 / 27 / 2023 PMC - LLaMA2 LLaMA 7 / 13B 79B 514 K QA 04 / 27 / 2023 Me - LLaMA LLaMA2 13 / 70B 129B 214 K QA, NER,