141 2 paper1. We compared the Rouge - 113 score for the summarization dataset PubMed, the accuracy score for three QA datasets, and the Macro - F1 score for the remaining datasets. With task - speciﬁc supervised ﬁne - tuning, Me - LLaMA modelssurpassed ChatGPT on 7out of 8 datasets and excelled GPT - 4 on 5 out of 8 datasets. In the zero - shot setting, Me - LLaMA models outperformed ChatGPT on 5 datasets ; but it fell short on 7 datasets, when compared with GPT - 4. It ’ s crucial to highlight that Me - LLaMA ’ s model size issigniﬁcantlysmaller — 13 / 70B parametersversusatleast175B forChatGPT and GPT - 4. Despite this size discrepancy, Me - LLaMA models have show - cased an impressive performance and a strong ability for supervised learning The zero - shot performance of various open source LLMs with chat capability Task Dataset Metric LLaMA2 - 13B - chat PMC - LLaMA - chat Medalpaca - 13B AlpaCare - 13B Me - LLaMA 13B - chat LLaMA2 - 70B - chat Me - LLaMA 70B - chat Question answering PubMedQA Accuracy 0. 546 0. 504 0. 238 0. 538 0. 700 0. 668 0. 768 Macro - F1 0. 457 0. 305 0. 192 0. 373 0. 504 0. 477 0. 557 MedQA Accuracy 0. 097 0. 207 0. 143 0. 304 0. 427 0. 376 0. 523 Macro - F1 0. 148 0. 158 0. 102 0. 281 0. 422 0. 367 0. 521 MedMCQA Accuracy 0. 321 0. 212 0. 205 0. 385 0. 449 0. 339 0. 539 Macro - F1 0. 243 0. 216 0. 164 0. 358 0. 440 0. 273 0. 538 EmrQA Accuracy 0. 001 0. 053 0. 000 0. 001 0. 048 0.