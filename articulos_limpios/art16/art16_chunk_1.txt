Proceedings of the BioNLP 2019 workshop, pages 58 – 65 Florence, Italy, August 1, 2019. Association for Computational Linguistics 58 Transfer Learning in Biomedical Natural Language Processing : An Evaluation of BERT and ELMo on Ten Benchmarking Datasets Yifan Peng Shankai Yan Zhiyong Lu National Center for Biotechnology Information National Library of Medicine, National Institutes of Health Bethesda, MD, USA { yifan. peng, shankai. yan, zhiyong. lu } @ nih. gov Abstract Inspired by the success of the General Lan - guage Understanding Evaluation benchmark, we introduce the Biomedical Language Un - derstanding Evaluation ( BLUE ) benchmark to facilitate research in the development of pre - training language representations in the biomedicine domain. The benchmark consists of ﬁve tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difﬁculties. We also evaluate several baselines based on BERT and ELMo and ﬁnd that the BERT model pre - trained on PubMed abstracts and MIMIC - III clinical notes achieves the best results. We make the datasets, pre - trained models, and codes pub - licly available at https : / / github. com / ncbi - nlp / BLUE _ Benchmark. 1 Introduction With the growing amount of biomedical informa - tion available in textual form, there have been signiﬁcant advances in the development of pre - training language representations that can be ap - plied to a range of different tasks in the biomedi - cal domain, such as pre - trained word embeddings, sentence embeddings, and contextual representa - tions ( Chiu, 2016 ; Chen, 2019 ; Peters, 2017 ; Lee, 2019 ; Smalheiser, 2019 ). In the general domain, we have recently ob - served that the General Language Understand - ing Evaluation ( GLUE ) benchmark ( Wang, 2018a ) has been successfully promoting the de - velopment of language representations of general purpose ( Peters, 2017 ; Radford, 2018 ; Devlin, 2019 ). To the best of our knowledge, however, there is no publicly available bench