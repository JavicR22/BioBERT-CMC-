, 2018 ) ; ShARe / CLEFE ( Leaman, 2015 ) ; DDI ( Zhang, 2018 ). Chem - Prot ( Peng, 2018 ) ; i2b2 ( Rink, 2011 ) ; HoC ( Du, 2019 ) ; MedNLI ( Romanov and Shivade, 2018 ). P : PubMed, P + M : PubMed + MIMIC - III Baseline performance on the BLUE task test sets. For named entity recognition, we used a Bi - LSTM - CRF implementation as a sequence tag - ger ( Huang, 2015 ; Si, 2019 ; Lample, 2016 ). Speciﬁcally, we concatenated the GloVe word embeddings ( Pennington, 2014 ), character embeddings, and ELMo embeddings of each token and fed the combined vectors into the sequence tagger to predict the label for each to - ken. The GloVe word embeddings11 and character embeddings have 100 and 25 dimensions, respec - tively. The hidden sizes of the Bi - LSTM are also set to 100 and 25 for the word and character em - beddings, respectively. For relation extraction and multi - label tasks, we followed the steps in ﬁne - tuning with BERT but used the averaged ELMo embeddings of all words in each sentence as the sentence embedding. 5 Benchmark results and discussion We pre - trained four BERT models : BERT - Base ( P ), BERT - Large ( P ), BERT - Base ( P + M ), BERT - Large ( P + M ) on PubMed abstracts only, and the combination of PubMed abstracts and clinical notes, respectively. We present performance on the main benchmark tasks in More de - tailed comparison is shown in the Appendix A. 11https : / / nlp. stanford. edu / projects / glove / Overall, our BERT - Base ( P + M ) that were pre - trained on both PubMed abstract and MIMIC - III achieved the best results across ﬁve tasks, even though it is only slightly better than the one pre - trained on PubMed abstracts only. Compared to the tasks in