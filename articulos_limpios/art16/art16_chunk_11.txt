the clinical domain and biomedical do - main, BERT - Base ( P + M ) is signiﬁcantly superior to other models. This demonstrates the importance of pre - training among different text genres. When comparing BERT pre - trained using the base settings against that using the large settings, it is a bit surprising that BERT - Base is better than BERT - Large except in relation extraction and document classiﬁcation tasks. Further analysis shows that, on these tasks, the average length of sentences is longer than those of others ( Ta - ble 1 ). In addition, BERT - Large pre - trained on PubMed and MIMIC is worse than other models overall. However, BERT - Large ( P ) performs the best in the multilabel task, even compared with the feature - based model utilizing enriched ontol - ogy ( Yan and Wong, 2017 ). This is partially be - cause the MIMIC - III data are relatively smaller than the PubMed abstracts and, thus, cannot pre - train the large model sufﬁciently. In the sentence similarity tasks, BERT - Base ( P + M ) achieves the best results on both datasets. Because the BIOSSES dataset is very small ( there 63 are only 16 sentence pairs in the test set ), all BERT models ’ performance was unstable. This prob - lem has also been noted in the work of Devlin when the model was evaluated on the GLUE benchmarking. Here, we obtained the best results by following the same strategy : selecting the best model on the development set after sev - eral runs. Other possible ways to overcome this issue include choosing the model with the best per - formance from multiple runs or averaging results from multiple ﬁne - tuned models. In the named entity recognition tasks, BERT - Base ( P ) achieved the best results on two biomedi - cal datasets, whereas BERT - Base ( P + M ) achieved the best results on the clinical dataset. In all cases, we observed that the winning model ob - tained higher recall than did the others. Given that we use the pre - deﬁned vocabulary in the original BERT and that this task relies heavily on the to - kenization, it is possible that using BERT as per - taining to a custom sentence piece tokenizer