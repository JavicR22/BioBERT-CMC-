the premise sentence entails or contradicts the hy - pothesis sentence. We use the standard overall ac - curacy to evaluate the performance. MedNLI is a collection of sentence pairs se - lected from MIMIC - III ( Romanov and Shivade, 2018 ). 9 Given a premise sentence and a hy - chemprot - corpus - biocreative - vi / 7https : / / www. i2b2. org / NLP / DataSets / 8https : / / www. cl. cam. ac. uk / / HoC. html 9https : / / physionet. org / physiotools / mimic - code / mednli / 61 pothesis sentence, two board - certiﬁed radiologists graded whether the task predicted whether the premise entails the hypothesis ( entailment ), con - tradicts the hypothesis ( contradiction ), or neither ( neutral ). We use the same training, development, and test sets in Romanov and Shivade ( Romanov and Shivade, 2018 ). 3. 6 Total score Following the practice in Wang ( 2018a ) and Lee, we use a macro - average of F1 - scores and Pearson scores to determine a system ’ s position. 4 Baselines For baselines, we evaluate several pre - training models as described below. The original code for the baselines is available at https : / / github. com / ncbi - nlp / NCBI _ BERT. 4. 1 BERT 4. 1. 1 Pre - training BERT BERT ( Devlin, 2019 ) is a contextualized word representation model that is pre - trained based on a masked language model, using bidirec - tional Transformers ( Vaswani, 2017 ). In this paper, we pre - trained our own model BERT on PubMed abstracts and clinical notes ( MIMIC - III ). The statistics of the text corpora on which BERT was pre - trained are shown in Corpus Words Domain PubMed abstract > 4, 000M Biomedical MIMIC - III > 500M Clinical Corpora We initialized BERT with pre - trained BERT provided by ( Devlin, 2019 ). We then con - tinue to pre - train the model