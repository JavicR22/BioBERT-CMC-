, using the listed cor - pora. We released our BERT - Base and BERT - Large models, using the same vocabulary, sequence length, and other conﬁgurations provided by De - vlin. Both models were trained with 5M steps on the PubMed corpus and 0. 2M steps on the MIMIC - III corpus. 4. 1. 2 Fine - tuning with BERT BERT is applied to various downstream text - mining tasks while requiring only minimal archi - tecture modiﬁcation. For sentence similarity tasks, we packed the sentence pairs together into a single sequence, as suggested in Devlin. For named entity recognition, we used the BIO tags for each token in the sentence. We considered the tasks similar to machine translation, as predict - ing the sequence of BIO tags from the input sen - tence. We treated the relation extraction task as a sen - tence classiﬁcation by replacing two named en - tity mentions of interest in the sentence with pre - deﬁned tags ( e. g., @ GENE ) ( Lee, 2019 ). For example, we used “ @ CHEMI - CAL binding. ” to replace the orig - inal sentence “ Citalopram protected against the RTI - 76 - induced inhibition of SERT binding. ” in which “ citalopram ” and “ SERT ” has a chemical - gene relation. For multi - label tasks, we ﬁne - tuned the model to predict multi - labels for each sentence in the document. We then combine the labels in one doc - ument and compare them with the gold - standard. Like BERT, we provided sources code for ﬁne - tuning, prediction, and evaluation to make it straightforward to follow those examples to use our BERT pre - trained models for all tasks. 4. 2 Fine - tuning with ELMo We adopted the ELMo model pre - trained on PubMed abstracts ( Peters, 2017 ) to accom - plish the BLUE tasks. 10 The output of ELMo em - beddings of each token is used as input for the ﬁne - tuning model. We retrieved the output states of both layers in ELMo and concatenated them into one vector for each word. We used the maxi - mum sequence length 128 for padding. The learn - ing rate was set to 0