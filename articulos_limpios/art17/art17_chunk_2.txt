##cal narratives ( e. g., physician notes ) have known differences in linguistic characteristics from both general text and non - clinical biomedical text, mo - tivating the need for specialized clinical BERT models. In this work, we build and publicly release ex - actly such an embedding model. 1 Furthermore, we demonstrate on several clinical NLP tasks the improvements this system offers over traditional BERT and BioBERT alike. In particular, we make the following contribu - tions : 1. We train and publicly release BERT - Base and BioBERT - ﬁnetuned models trained on both all clinical notes and only discharge sum - maries. 2 2. We demonstrate that using clinical speciﬁc contextual embeddings improves both upon general domain results and BioBERT results across 2 well established clinical NER tasks and one medical natural language inference task ( i2b2 2010 ( Uzuner, 2011 ), i2b2 2012 ( Sun, 2013a, b ), and MedNLI ( Romanov and Shivade, 2018 ) ). On 2 de - identiﬁcation ( de - ID ) tasks, i2b2 2006 ( Uzuner, 2007 ) and i2b2 2014 ( Stubbs, 2015 ; Stubbs and Uzuner, 2015 ), gen - eral BERT and BioBERT outperform clinical BERT and we argue that fundamental facets of the de - ID context motivate this lack of per - formance. 2 Related Work Contextual Embeddings in General Tradi - tional word - level vector representations, such as word2vec ( Mikolov, 2013 ), GloVe ( Penning - ton, 2014 ), and fastText ( Bojanowski, 1github. com / EmilyAlsentzer / clinicalBERT 2Discharge summaries are commonly used in downstream tasks. arXiv : 1904. 03323v3 [ cs. CL ] 20 Jun 2019 2017 ), express all possible meanings of a word as a single vector representation and cannot disam - biguate the word senses based on the surround - ing context. Over the last two years, ELMo ( Pe - ters, 2018 ) and BERT ( Devlin, 2018 ) present strong