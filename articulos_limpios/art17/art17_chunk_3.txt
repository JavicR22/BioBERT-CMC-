solutions that can provide contex - tualized word representations. By pre - training on a large text corpus as a language model, ELMo can create a context - sensitive embedding for each word in a given sentence, which will be fed into downstream tasks. Compared to ELMo, BERT is deeper and contains much more parameters, thus possessing greater representation power. More im - portantly, rather than simply providing word em - beddings as features, BERT can be incorporated into a downstream task and gets ﬁne - tuned as an integrated task - speciﬁc architecture. BERT has, in general, been found to be supe - rior to ELMo and far superior to non - contextual embeddings on a variety of tasks, including those in the clinical domain ( Si, 2019 ). For this reason, we only examine BERT here, rather than including ELMo or non - contextual embed - ding methods. Contextual Clinical & Biomedical Embeddings Several works have explored the utility of con - textual models in the clinical and biomedical do - mains. BioBERT ( Lee, 2019 ) trains a BERT model over a corpus of biomedical research arti - cles sourced from PubMed3 article abstracts and PubMed Central4 article full texts. They ﬁnd the speciﬁcity offered by biomedical texts trans - lated to improved performance on several biomed - ical NLP tasks, and fully release their pre - trained BERT model. On clinical text, ( Khin, 2018 ) uses a general - domain pretrained ELMo model towards the task of clinical text de - identiﬁcation, report - ing near state - of - the - art performance on the i2b2 2014 task ( Stubbs and Uzuner, 2015 ; Stubbs, 2015 ) and state of the art performance on several axes of the HIPAA PHI dataset. Two works that we know of train contextual em - bedding models on clinical corpora. ( Zhu, 2018 ) trains an ELMo model over a corpus of mixed clinical discharge summaries, clinical radiology notes and medically oriented wikipedia articles, then demonstrates improved performance on the i2b2 2010 task ( Uzuner, 3https : / / www. n