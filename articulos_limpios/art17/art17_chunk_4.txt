##cbi. nlm. nih. gov / pubmed / 4https : / / www. ncbi. nlm. nih. gov / pmc / 2011 ). They release a pre - trained ELMo model along with their work, enabling further clinical NLP research to work with these powerful con - textual embeddings. ( Si, 2019 ), released in late February 2019, train a clinical note corpus BERT language model and uses complex task - speciﬁc models to yield im - provements over both traditional embeddings and ELMo embeddings on the i2b2 2010 and 2012 tasks ( Sun, 2013b, a ) and the SemEval 2014 task 7 ( Pradhan, 2014 ) and 2015 task 14 ( El - hadad ) tasks, establishing new state - of - the - art results on all four corpora. However, this work neither releases their embeddings for the larger community nor examines the performance oppor - tunities offered by ﬁne - tuning BioBERT with clin - ical text or by training note - type speciﬁc embed - ding models, as we do. 3 Methods In this section, we ﬁrst describe our clinical text dataset, the details of the BERT training proce - dure, and ﬁnally the speciﬁc tasks we examine. 3. 1 Data We use clinical text from the approximately 2 mil - lion notes in the MIMIC - III v1. 4 database ( John - son, 2016 ). Details of our text pre - processing procedure can be found in Appendix A. Note that while some of our tasks use a small subset of MIMIC notes in their corpora, we do not try to ﬁl - ter these notes out of our BERT pre - training proce - dure. We expect the bias this induces is negligible given the relative sizes of the two corpora. We train two varieties of BERT on MIMIC notes : Clinical BERT, which uses text from all note types, and Discharge Summary BERT, which uses only discharge summaries in an effort to tai - lor the corpus to downstream tasks ( which often largely use discharge summaries ). Note that we train our clinical B