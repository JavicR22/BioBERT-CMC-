##ERT instantia - tions on all notes of the appropriate type ( s ), with - out regard for whether or not any individual note appeared in any of the train / test sets for the vari - ous tasks we use ( two of which use a small subset of MIMIC notes either partially or completely as their backing corpora ). We feel this has a negligi - ble impact given the dramatically larger size of the entire MIMIC corpus relative to the various task corpora. 3. 2 BERT Training In this work, we aim to provide the pre - trained embeddings as a community resource, rather than demonstrate technical novelty in the training pro - cedure, and accordingly our BERT training pro - cedure is completely standard. As such, we have relegated speciﬁcs of the training procedure to Ap - pendix B. We trained two BERT models on clinical text : 1 ) Clinical BERT, initialized from BERT - Base, and 2 ) Clinical BioBERT, initialized from BioBERT. For all downstream tasks, BERT mod - els were allowed to be ﬁne - tuned, then the out - put BERT embedding was passed through a single linear layer for classiﬁcation, either at a per - token level for NER or de - ID tasks or applied to the sen - tinel “ begin sentence ” token for MedNLI. Note that this is a substantially lower capacity model than, for example, the Bi - LSTM layer used in ( Si, 2019 ). This reduced capacity potentially limits performance on downstream tasks, but is in line with our goal of demonstrating the efﬁcacy of clinical - speciﬁc embeddings and releasing a pre - trained BERT model for these embeddings. We did not experiment with more complex representa - tions as our goal is not to necessarily surpass state - of - the - art performances on these tasks. Computational Cost Pre - processing and train - ing BERT on MIMIC notes took signiﬁcant com - putational resources. We estimate that our en - tire embedding model procedure took roughly 17 - 18 days of computational runtime using a single GeForce GTX TITAN X 12 GB GPU ( and signiﬁ