a result, they present signiﬁcantly different text distribu - tions than traditionally de - identiﬁed text ( such as MIMIC notes ) which will instead present sentinel “ PHI ” symbols at locations where PHI was re - moved. 4 Results & Discussions In this section, we will ﬁrst describe quantitative comparisons of the various BERT models on the clinical NLP tasks we considered, and second de - scribe qualitative evaluations of the differences be - tween Clinical - and Bio - BERT. Clinical NLP Tasks Full results are shown in On three of the ﬁve tasks ( MedNLI, i2b2 2010, and i2b2 2012 ), clinically ﬁne - tuned BioBERT shows improvements over BioBERT or general BERT. Notably, on MedNLI, clinical BERT actually yields a new state of the art, yield - ing a performance of 82. 7 % accuracy as compared to the prior state of the art of 73. 5 % ( Romanov and Shivade, 2018 ) obtained via the InferSent model ( Conneau, 2017 ). However, on our two de - ID tasks, i2b2 2006 and i2b2 2014, clinical BERT offers no improvements over Bio - or gen - eral BERT. This is actually not surprising, and is instead, we argue, a direct consequence of the na - Model MedNLI i2b2 2006 i2b2 2010 i2b2 2012 i2b2 2014 BERT 77. 6 % 93. 9 83. 5 75. 9 92. 8 BioBERT 80. 8 % 94. 8 86. 5 78. 9 93. 0 Clinical BERT 80. 8 % 91. 5 86. 4 78. 5 92. 6 Discharge Summary BERT 80. 6 % 91. 9 86. 4 78. 4 92. 8 Bio + Clinical BERT 82. 7 % 94. 7 87. 2 78. 9 92. 5 Bio + Discharge Summary BERT 82. 7 % 94. 8 87. 8 78. 9 92. 7 Accuracy ( MedNLI ) and Exact F1 score ( i2b2 ) across various clinical NLP tasks. Model Disease