arXiv : 1903. 10676v3 [ cs. CL ] 10 Sep 2019 SCIBERT : A Pretrained Language Model for Scientiﬁc Text Iz Beltagy Kyle Lo Arman Cohan Allen Institute for Artiﬁcial Intelligence, Seattle, WA, USA { beltagy, kylel, armanc } @ allenai. org Abstract Obtaining large - scale annotated data for NLP tasks in the scientiﬁc domain is challeng - ing and expensive. We release SCIBERT, a pretrained language model based on BERT ( Devlin, 2019 ) to address the lack of high - quality, large - scale labeled scientiﬁc data. SCIBERT leverages unsupervised pretraining on a large multi - domain corpus of scientiﬁc publications to improve perfor - mance on downstream scientiﬁc NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classiﬁcation and dependency parsing, with datasets from a variety of scientiﬁc domains. We demon - strate statistically signiﬁcant improvements over BERT and achieve new state - of - the - art results on several of these tasks. The code and pretrained models are available at https : / / github. com / allenai / scibert /. 1 Introduction The exponential increase in the volume of scien - tiﬁc publications in the past decades has made NLP an essential tool for large - scale knowledge extraction and machine reading of these docu - ments. Recent progress in NLP has been driven by the adoption of deep neural models, but train - ing such models often requires large amounts of labeled data. In general domains, large - scale train - ing data is often possible to obtain through crowd - sourcing, but in scientiﬁc domains, annotated data is difﬁcult and expensive to collect due to the ex - pertise required for quality annotation. As shown through ELMo ( Peters, 2018 ), GPT ( Radford, 2018 ) and BERT ( Devlin, 2019 ), unsupervised pre - training of language models on large corpora signiﬁcantly improves performance on many NLP tasks. These