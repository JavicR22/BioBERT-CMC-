models return contextualized embeddings for each token which can be passed into minimal task - speciﬁc neural architectures. Leveraging the success of unsupervised pretrain - ing has become especially important especially when task - speciﬁc annotations are difﬁcult to obtain, like in scientiﬁc NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu - tions : ( i ) We release SCIBERT, a new resource demon - strated to improve performance on a range of NLP tasks in the scientiﬁc domain. SCIBERT is a pre - trained language model based on BERT but trained on a large corpus of scientiﬁc text. ( ii ) We perform extensive experimentation to investigate the performance of ﬁnetuning ver - sus task - speciﬁc architectures atop frozen embed - dings, and the effect of having an in - domain vo - cabulary. ( iii ) We evaluate SCIBERT on a suite of tasks in the scientiﬁc domain, and achieve new state - of - the - art ( SOTA ) results on many of these tasks. 2 Methods Background The BERT model architecture ( Devlin, 2019 ) is based on a multilayer bidi - rectional Transformer ( Vaswani, 2017 ). In - stead of the traditional left - to - right language mod - eling objective, BERT is trained on two tasks : pre - dicting randomly masked tokens and predicting whether two sentences follow each other. SCIB - ERT follows the same architecture as BERT but is instead pretrained on scientiﬁc text. Vocabulary BERT uses WordPiece ( Wu, 2016 ) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with BERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vo - cabulary on our scientiﬁc corpus using the Sen - tence