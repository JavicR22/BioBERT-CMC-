##Piece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The re - sulting token overlap between BASEVOCAB and SCIVOCAB is 42 %, illustrating a substantial dif - ference in frequently used words between scien - tiﬁc and general domain texts. Corpus We train SCIBERT on a random sample of 1. 14M papers from Semantic Scholar ( Ammar, 2018 ). This corpus consists of 18 % papers from the computer science domain and 82 % from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences ( 2, 769 tokens ) resulting in a corpus size of 3. 17B tokens, similar to the 3. 3B tokens on which BERT was trained. We split sentences using ScispaCy ( Neumann, 2019 ), 2 which is optimized for scientiﬁc text. 3 Experimental Setup 3. 1 Tasks We experiment on the following core NLP tasks : 1. Named Entity Recognition ( NER ) 2. PICO Extraction ( PICO ) 3. Text Classiﬁcation ( CLS ) 4. Relation Classiﬁcation ( REL ) 5. Dependency Parsing ( DEP ) PICO, like NER, is a sequence labeling task where the model extracts spans describing the Partici - pants, Interventions, Comparisons, and Outcomes in a clinical trial paper ( Kim, 2011 ). REL is a special case of text classiﬁcation where the model predicts the type of relation expressed be - tween two entities, which are encapsulated in the sentence by inserted special tokens. 3. 2 Datasets For brevity, we only describe the newer datasets here, and refer the reader to the