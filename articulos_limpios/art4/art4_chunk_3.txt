[ 6 – 9 ] are de - veloped based on statistical learning methods that rose in the 1990s. The basic idea is to build the word prediction model based on the Markov assumption, e. g., predicting the next word based on the most recent context. The SLMs with a fixed context length n are also called n - gram language models, e. g., bigram and trigram language models. SLMs have been widely applied to enhance task performance in information retrieval ( IR ) and natural language processing ( NLP ) [ 12 – 14 ]. However, they often suffer from the curse of dimensionality : it is difficult to accurately estimate high - order language models since an exponential number of transition probabilities need to be estimated. Thus, specially designed smoothing strategies such as back - off estimation and Good – Turing estimation have been introduced to alleviate the data sparsity problem. Neural language models ( NLM ). NLMs charac - terize the probability of word sequences by neural networks, e. g., multi - layer perceptron ( MLP ) and recurrent neural net - works ( RNNs ). As a remarkable contribution, the work in introduced the concept of distributed representation of words and built the word prediction function conditioned on the aggregated context features ( i. e., the distributed word vectors ). By extending the idea of learning effective features for text data, a general neural network approach was developed to build a unified, end - to - end solution for arXiv : 2303. 18223v16 [ cs. CL ] 11 Mar 2025 2 2018 2019 2020 2021 2022 2023 Time 0 2000 4000 6000 8000 10000 GPT - 1 BERT GPT - 2 T5 GPT - 3 Codex InstructGPT ChatGPT LLaMA GPT - 4 ( a ) Query = ” Language Model ” 2020 2021 2022 2023 Time 0 250 500 750 1000 1250 1500 1750 T5 GPT - 3 Codex InstructGPT ChatGPT LLaMA GPT - 4 ( b ) Query = ” Large Language Model ” The trends of the cumulative numbers of arXiv papers that contain the keyphrases “ language model ” ( since June 2018 ) and “ large language model ” ( since October 2019 ), respectively. The statistics