##TM ) network ( instead of learning fixed word representations ) and then fine - tuning the biLSTM network according to specific downstream tasks. Furthermore, based on the highly parallelizable Transformer architecture with self - attention mechanisms, BERT was proposed by pre - training bidirectional language models with specially designed pre - training tasks on large - scale unlabeled cor - pora. These pre - trained context - aware word representations are very effective as general - purpose semantic features, which have largely raised the performance bar of NLP tasks. This study has inspired a large number of follow - up work, which sets the “ pre - training and fine - tuning ” learning paradigm. Following this paradigm, a great number of stud - ies on PLMs have been developed, introducing either differ - ent architectures ( e. g., GPT - 2 and BART ) or improved pre - training strategies [ 27 – 29 ]. In this paradigm, it often requires fine - tuning the PLM for adapting to different downstream tasks. Large language models ( LLM ). Researchers find that scaling PLM ( e. g., scaling model size or data size ) often leads to an improved model capacity on downstream tasks ( i. e., following the scaling law ). A number of studies 3 have explored the performance limit by training an ever larger PLM ( e. g., the 175B - parameter GPT - 3 and the 540B - parameter PaLM ). Although scaling is mainly conducted in model size ( with similar architectures and pre - training tasks ), these large - sized PLMs display different behaviors from smaller PLMs ( e. g., 330M - parameter BERT and 1. 5B - parameter GPT - 2 ) and show surprising abilities ( called emer - gent abilities ) in solving a series of complex tasks. For example, GPT - 3 can solve few - shot tasks through in - context learning, whereas GPT - 2 cannot do well. Thus, the research community coins the term “ large language models ( LLM ) ” 1 for these large - sized PLMs [ 32 – 35 ], which attract increasing research attention ( See A remarkable application of LLMs is ChatGPT2 that adapts the LLMs from the GPT series for dialogue, which presents an amazing conversation ability with humans. We can observe a sharp increase of the arXiv