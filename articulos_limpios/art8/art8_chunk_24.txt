performances of the predictors was made using different metrics ( mean absolute error either general and considering only those mutations that have an experimental G < 4 kcal / mol ; Pearson ’ s and Spearman ’ s correlation coefficients ; average of the recall for neutral mutations and for destabilizing mutations ; AUC, i. e. the area under the receiver operating characteristic ( ROC ) curve ). The authors classified ELASPIC as the method with Downloaded from https : / / academic. oup. com / bib / article / 22 / 3 / bbaa074 / 5850907 by guest on 30 October 2025 Predicting the stability of mutant proteins by computational approaches : an overview 13 the best accuracy in predicting the G of individual muta - tions, despite a mean square error close to 1 kcal / mol. Rosetta was the best performing method in terms of distinguishing between neutral and destabilizing mutations. In their assess - ment, the authors also found that predictors based on protein structures achieved a relatively little improvement with respect to sequence - based predictors, with a higher computational cost. Finally, a very recent assessment by Fang focused on the per - formances of five very popular machine learning - based methods ( MUpro, STRUM, mCSM, DUET, I - Mutant 2. 0 ). Fang evaluated these predictors with several statistical metrics such as the per - cent of inconsistence of predictions, the percent of correctly pre - dicted signs and the AUC. This assessment has highlighted that most of 70 % of predictions of all predictors are inconsistent ( e. g. a mutation and its reverse hypothetical mutation have the same sign, meaning that a mutant protein is simultaneously more and less stable than the wild type ). mCSM resulted the one with the worst performance. Moreover, while the accuracy in predicting the change in stability for the mutation was generally above 80 %, this accuracy dropped down to 30 % for the corresponding inverse hypothetical mutation. These data are in agreement with the problem of overtraining of all these predictors toward a biased dataset. In agreement with previous assessments, the reasons for these discouraging results were found essentially in the data and features used to train the algorithms. A comment to the Fang assessment has been very recently reported to highlight that INPS - MD, a stability predictor developed to specifically