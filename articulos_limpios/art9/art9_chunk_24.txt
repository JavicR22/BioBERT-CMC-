a clear understanding of how the model reached its conclusions, thus improving transparency and trustworthiness. Although NSSC is LLM - agnostic, as a proof of concept, we implemented this step using the OpenAI API. 5 This API involves determining the appropriate model and includes conﬁguration parameters such as the maximum number of tokens and the temperature of the language model. The opti - mal conﬁguration of these parameters is essential to achieve the desired behavior. In Section5, the effects of combining different possibili - ties to assess their impact on the LLM ’ s outcome. Although the full text of the note in the prompt could offer the potential 5 https : / / openai. com / blog / openai - api for richer interactions with the LLM, several challenges must be effectively addressed to make this approach successful. Computational costs : LLMs, especially those hosted in the cloud, can have computational costs associated with processing large amounts of text. The more tokens you send, the more resources are required for analysis. Response time : Sending lengthy prompts may result in longer response times from the LLM. Although this is not a real - time issue, it is something to consider. Privacy and sensitive information : Sending complete notes may involve sharing sensitive or private informa - tion with external language model services, which raises privacy concerns. Model capacity limits : Some language models have input limits and may have a maximum token limit per request, and sending very long requests may result in truncation or incomplete processing. Algorithm 3 Medical entity disambiguation. 1 : Input : ( e, le, t ′ e ) 2 : Output : ( e, t ′ ′ e ) 3 : procedure Disambiguate ( ( e, le, t ′ e ) 4 : for each t in set t ′ e do 5 : Retrieve UMLS deﬁnition of t 6 : end for 7 : Set up the query with format pLLM and parameters : le, t ′ e 8 : Query pLLM to LLM 9 : Process the query 10 : if LLM return etranslated then 11 : LinkEntities ( etranslated ) 12 : else 13 : return ( e, t ′ ′ e ) 14 : end if 15 : end procedure The best prompt for our approach is illustrated in which is