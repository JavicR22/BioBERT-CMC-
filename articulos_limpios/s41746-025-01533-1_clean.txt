npj | digital medicine Article Published in partnership with Seoul National University Bundang Hospital https://doi.org/10.1038/s41746-025-01533-1 Medical foundation largelanguage models for comprehensive text analysis and beyond Check for updates Qianqian Xie1,5, Qingyu Chen1,5, Aokun Chen2,5, Cheng Peng2, Yan Hu3, Fongci Lin1, Xueqing Peng1, Jimin Huang1, Jeffrey Zhang1, Vipina Keloth1, Xinyu Zhou1, Lingfei Qian1, Huan He1, Dennis Shung1,4, Lucila Ohno-Machado1, Yonghui Wu2, Hua Xu1 & Jiang Bian2 Recent advancements in large language models (LLMs) show signiﬁcant potential in medical applications but are hindered by limited specialized medical knowledge. We present Me-LLaMA, a family of open-source medical LLMs integrating extensive domain-speciﬁc knowledge with robust instruction-following capabilities. Me-LLaMA is developed through continual pretraining and instruction tuning of LLaMA2 models using diverse biomedical and clinical data sources (e.g., biomedical literature and clinical notes). We evaluated Me-LLaMA on six text analysis tasks using 12 benchmarks (e.g., PubMedQA and MIMIC-CXR) and assessed its clinical utility in complex case diagnosis through automatic and human evaluations. Me-LLaMA outperforms existing open medical LLMs in zero-shot and supervised settings and surpasses ChatGPT and GPT-4 after task-speciﬁc instruction tuning for most text analysis tasks. Its performance is also comparable to ChatGPT and GPT-4 for diagnosing complex clinical cases. Our ﬁndings highlight the importance of combining domain-speciﬁc continual pretraining with instruction tuning to enhance performance in medical LLMs. Large language models (LLMs) have shown great potential in improving medical applications such as clinical documentation, diagnostic accuracy, andpatientcaremanagement1–3.However,general-domainLLMsoftenlack specialized medical knowledge because they are primarily trained on non- medical datasets4, limiting their effectiveness in healthcare settings. Although commercial LLMs, such as ChatGPT and GPT-44, offer advanced capabilities, their closed-source nature restricts the ﬂexible customization and accessibility required for medical use. This limitation has spurred the researchtowardsdevelopingopen-sourceLLMssuchasLLaMA5,6;Yetthese models still fall short due to their general-domain training7–9. To address these challenges, researchers have explored strategies to develop domain-speciﬁc LLMs for the medical domain. Instruction ﬁne- tuning of general-domain models, as seen in MedAlpaca3, ChatDoctor10, and AlpaCare11, attempts to enhance medical capabilities but is limited by thebasemodels’lackofspecializedknowledge;instructionﬁne-tuningalone cannot compensate for this deﬁciency. Training models from scratch using medicalcorpora,exempliﬁedbyGatorTronGPT8,overcomesthislimitation but demands substantial computational resources and time. A more cost- effective alternative is continual pretraining, enabling models to acquire specialized medical knowledge while leveraging existing model archi- tectures; notable examples include PMC-LLaMA2, Meditron9, and Clinical LLaMA12. Despite these advances, existing LLMs of continual pretraining in the medical domain exhibit notable limitations: Although both domain knowledge and instruction-following capabilities are crucial, only PMC- LLaMA2 has combined continual pretraining with instruction ﬁne-tuning, revealingagapinleveragingthesynergybetweenthesetwoaspects.Only one model (Clinical LLaMA) used clinical notes from electronic health records, which is crucial for real-world clinical applications as it provides context-speciﬁc information from direct patient care. None of the existing modelsusedbothbiomedicalliteratureandclinicalnotes,whichisoneofthe goals of this project. Due to the limited medical datasets utilized for model development, these models still lack essential domain knowledge, which hampers their effectiveness. By combining biomedical literature and 1Department of Biomedical Informatics and Data Science, Yale School of Medicine, Yale University, New Haven, CT, USA. 2Department of Health Outcomes and Biomedical Informatics, College of Medicine, University of Florida, Gainesville, FL, USA. 3School of Biomedical Informatics, University of Texas Health Science, Center at Houston, Houston, TX, USA. 4Department of Medicine (Digestive Diseases), Yale School of Medicine, Yale University, New Haven, CT, USA. 5These authors contributed equally: Qianqian Xie, Qingyu Chen, Aokun Chen. e-mail: hua.xu@yale.edu; bianji@iu.edu npj Digital Medicine | 8:141 1 1234567890():,; 1234567890():,; clinical notes, we generated the largest biomedical pre-training dataset (129B tokens), compared to the previous efforts (i.e., 79B tokens in PMC- LLaMA as the highest). Evaluations have predominantly centered on medical question-answering (QA) tasks, lacking comprehensive assess- ments on the generalizability of those foundation models across diverse medical tasks. To overcome these limitations, we present Me-LLaMA, a novel family of open-source medical large language models that uniquely integrate extensive domain-speciﬁc knowledge with robust instruction-following capabilities. Me-LLaMA comprises foundation models (Me-LLaMA 13B and 70B) and their chat-enhanced versions, developed through compre- hensive continual pretraining and instruction tuning of LLaMA26 models. Leveraging an extensive medical dataset—combining 129 billion pretrain- ingtokensand214,000instructionsamplesfromscientiﬁcliterature,clinical guidelines, and electronic health record clinical notes—Me-LLaMA excels across a wide spectrum of medical text analysis and real-world clinical tasks. Prior studies2,3,7–12 have primarily focused on evaluating the QA task. For example, PMC-LLaMA2 and Meditron9 evaluated their model performance on medical QA tasks derived from domain-speciﬁc literature, while MedAlpaca3 and ChatDoctor10 focused on conversational QA. In contrast, we conduct a comprehensive evaluation covering six critical tasks — question answering, relation extraction, named entity recognition, text classiﬁcation, text summarization, and natural language inference—across twelve datasets from both biomedical and clinical domains. Our results demonstrate that Me-LLaMA not only surpasses existing open-source medical LLMs in both zero-shot and supervised settings but also, with task- speciﬁc instruction tuning, outperforms leading commercial LLMs such as ChatGPT on seven out of eight datasets and GPT-4 on ﬁve out of eight datasets. Furthermore, to evaluate Me-LLaMA’s potential clinical utility, we assessed the models on complex clinical case diagnosis tasks, comparing their performance with other commercial LLMs using both automatic and human evaluations. Ourﬁndingsindicatethat Me-LLaMA’s performance is comparabletothatofChatGPTandGPT-4,despitetheirsubstantiallylarger model sizes. Our ﬁndings underscore the importance of combining domain- speciﬁc continual pretraining with instruction tuning to develop effective large language models for the medical domain. Recognizing the signiﬁcant resources required, we have publicly released our Me-LLaMA models on PhysioNet under appropriate Data Use Agreements (DUAs) to lower bar- riersandfosterinnovationwithinthemedicalAIcommunity.Alongsidethe models, we provide benchmarks and evaluation scripts on GitHub to facilitate further development. We anticipate that these contributions will beneﬁt researchers and practitioners alike, advancing this critical ﬁeld toward more effective and accessible medical AI applications. Results Overall performance of medical text analysis compares the performance of our Me-LLaMA 13/70B foundation modelsagainst otheropen LLMsin the supervised setting. The performance of Meditron 70B on the PubMedQA, MedQA, and MedMCQA datasets is cited from the meditron paper to have a fair comparison. We can observe that the Me-LLaMA 13B model surpassed the similar-sized medical foun- dation model PMC-LLaMA 13B on 11 out of 12 datasets and outperformed the general foundation model LLaMA2 13B on 10 out of 12 datasets. Moreover,itisnoticedthattheMe-LLaMA13Bmodelwascompetitivewith LLaMA2 70B and Meditron 70B, which have signiﬁcantly larger parameter sizes, on 8 out of 12 datasets. As for 70B models, Me-LLaMA 70B achieved the best performance on 9 out of 12 datasets, when benchmarked against LLaMA2 70B and Meditron 70B. shows the zero-shot performance of Me-LLaMA chat models and other instruction-tuned open LLMs with chat ability on various tasks. Among 13B models, Me-LLaMA 13B-chat outperformed LLaMA2 13B-chat, PMC-LLaMA-chat, Medalpaca 13B in almost all 12 datasets. Me-LLaMA outperformed AlpaCare-13B in 9 out of 12 datasets. Among models with 70B parameters, Me-LLaMA 70B-chat consistently outperformed LLaMA2-70B-chat on 11 out of 12 datasets. It is worth noting that Me-LLaMA13B-chat showed better performance than LLaMA2-70B-chat—a model with a signiﬁcantly larger parameter size—on 6outof12datasetsandwascompetitivewiththeLLaMA2-70B-chatin3out of 6 remaining datasets. zero-shot and supervised learning setting, against ChatGPT and GPT-4. Due to privacy concerns, which preclude the transmission of clinical datasets with patient information to ChatGPT and GPT-4, we conducted our comparison across 8 datasets that are not subject to these limitations. The results of ChatGPTandGPT-4onthreeQAdatasetsarereferencedfromtheOpenAI’s The supervised ﬁne-tuning performance of various open source LLMs on six tasks Task Dataset Metric LLaMA2 13B PMC-LLaMA 13B Me-LLaMA 13B LLaMA2 70B Meditron 70B Me-LLaMA 70B Question answering PubMedQA Acc 0.800 0.778 0.802 0.800 0.800 0.814 Macro-F1 0.560 0.544 0.562 0.560 – 0.572 MedQA Acc 0.467 0.456 0.493 0.598 0.607 0.623 Macro-F1 0.465 0.454 0.487 0.595 – 0.621 MedMCQA Acc 0.527 0.548 0.557 0.626 0.651 0.643 Macro-F1 0.524 0.545 0.551 0.625 – 0.640 EmrQA Acc 0.789 0.810 0.857 0.847 0.850 0.854 F1 0.730 0.738 0.751 0.751 0.751 0.751 Named entity recognition i2b2 Macro-F1 0.904 0.901 0.906 0.913 0.908 0.910 Relation extraction DDI Macro-F1 0.622 0.622 0.559 0.746 0.737 0.779 Classiﬁcation HoC Macro-F1 0.696 0.422 0.684 0.818 0.702 0.841 MTsample Macro-F1 0.430 0.345 0.451 0.458 0.284 0.544 Summarization PubMed R-L 0.191 0.091 0.197 0.211 0.197 0.209 BERTS 0.663 0.516 0.679 0.689 0.677 0.700 MIMIC-CXR R-L 0.437 0.139 0.453 0.440 0.458 0.476 BERTS 0.816 0.694 0.821 0.813 0.824 0.828 Natural language inference BioNLI Macro-F1 0.409 0.332 0.447 0.447 0.444 0.566 MedNLI Macro-F1 0.881 0.868 0.903 0.884 0.897 0.916 BERTS means BERTScore28. https://doi.org/10.1038/s41746-025-01533-1 Article npj Digital Medicine | 8:141 2 paper1. We compared the Rouge-113 score for the summarization dataset PubMed, the accuracy score for three QA datasets, and the Macro-F1 score for the remaining datasets. With task-speciﬁc supervised ﬁne-tuning, Me- LLaMA modelssurpassed ChatGPT on 7out of 8 datasets and excelled GPT- 4 on 5 out of 8 datasets. In the zero-shot setting, Me-LLaMA models outperformed ChatGPT on 5 datasets; but it fell short on 7 datasets, when compared with GPT-4. It’s crucial to highlight that Me-LLaMA’s model size issigniﬁcantlysmaller—13/70B parametersversusatleast175B forChatGPT and GPT-4. Despite this size discrepancy, Me-LLaMA models have show- cased an impressive performance and a strong ability for supervised learning The zero-shot performance of various open source LLMs with chat capability Task Dataset Metric LLaMA2- 13B-chat PMC- LLaMA- chat Medalpaca-13B AlpaCare-13B Me-LLaMA 13B-chat LLaMA2- 70B-chat Me-LLaMA 70B-chat Question answering PubMedQA Accuracy 0.546 0.504 0.238 0.538 0.700 0.668 0.768 Macro-F1 0.457 0.305 0.192 0.373 0.504 0.477 0.557 MedQA Accuracy 0.097 0.207 0.143 0.304 0.427 0.376 0.523 Macro-F1 0.148 0.158 0.102 0.281 0.422 0.367 0.521 MedMCQA Accuracy 0.321 0.212 0.205 0.385 0.449 0.339 0.539 Macro-F1 0.243 0.216 0.164 0.358 0.440 0.273 0.538 EmrQA Accuracy 0.001 0.053 0.000 0.001 0.048 0.050 0.119 F1 0.098 0.304 0.040 0.198 0.307 0.251 0.346 Named entity recognition i2b2 Macro-F1 0.143 0.091 0.000 0.173 0.166 0.321 0.329 Relation extraction DDI Macro-F1 0.090 0.147 0.058 0.110 0.214 0.087 0.283 Classiﬁcation HoC Macro-F1 0.228 0.184 0.246 0.267 0.335 0.309 0.544 MTsample Macro-F1 0.133 0.083 0.003 0.273 0.229 0.254 0.384 Summarization PubMed Rouge-L 0.161 0.028 0.014 0.167 0.116 0.192 0.169 BERTS 0.671 0.128 0.117 0.671 0.445 0.684 0.678 MIMIC-CXR Rouge-L 0.144 0.139 0.010 0.134 0.400 0.131 0.418 BERTS 0.704 0.694 0.502 0.702 0.797 0.696 0.787 Natural language inference BioNLI Macro-F1 0.173 0.159 0.164 0.170 0.195 0.297 0.436 MedNLI Macro-F1 0.412 0.175 0.175 0.275 0.472 0.515 0.675 Performance comparison of Me-LLaMA models with ChatGPT and GPT-4. The ﬁgure presents the zero-shot performance of Me-LLaMA (Me-LLaMA zero-shot) alongside its supervised learning performance (Me-LLaMA task-speciﬁc), compared against the zero-shot performance of ChatGPT and GPT-4 across 8 datasets. https://doi.org/10.1038/s41746-025-01533-1 Article npj Digital Medicine | 8:141 3 and zero-shot learning across a broad spectrum of medical tasks, under- scoring its efﬁciency and potential in the ﬁeld. Performance of complex clinical case diagnosis shows the top-K (1 K 5) accuracy of Me-LLaMA-70B-chat, ChatGPT, GPT-4, and LLaMA2-70B-chat, in the complex clinical case diagnosis task. We can see Me-LLaMA-70B-chat model achieved com- parable performance with GPT-4 and ChatGPT and signiﬁcantly outper- forms LLaMA2-70B-chat. The human evaluation result in again shows that Me-LLaMA-70B-chat outperformed GPT-4 in both top-1 and top-5 accuracy. These results demonstrated the potential of Me-LLaMA models for challenging clinical applications. Impact of continual pretraining and instruction tuning demonstrates the impact of continual pre-training and instruction tuning on zero-shot performance across medical NLP tasks. It clearly demonstrates that both continual pre-training and instruction tuning sig- niﬁcantly enhanced the zero-shot capabilities of models. Instruction tuning aloneprovidessigniﬁcantperformanceimprovementsoverthebaseLLaMA2 models, as seen in LLaMA2 13B, where accuracy on PubMedQA increases from0.216 to 0.436. This suggests that instruction tuning is highly effective in enhancing the model’s ability to follow task-speciﬁc prompts. In contrast, continual pre-training on medical data yields relatively modest improve- ments, particularly for smaller models. Me-LLaMA 13B shows only slight gains over LLaMA2 13B, likely due to the smaller scale of domain-speciﬁc pre-training data compared to LLaMA2’s original training corpus, which exceeds 2 T tokens. Additionally, continual pre-training may not provide as strong of a task-speciﬁc signal as instruction tuning, limiting its impact in zero-shot settings. However, for larger models like Me-LLaMA 70B, con- tinual pre-training results in more notable improvements, with performance gains ranging from 2.1% to 55% across various datasets, demonstrating its value in capturing specialized domain knowledge. The best results are con- sistently achieved when both continual pre-training and instruction tuning are applied together, as seen in Me-LLaMA-70B-chat, which outperforms all other conﬁgurations. This indicates that while instruction tuning is the most efﬁcient approach for improving task performance, continual pre-training provides a complementary boost, particularly for larger models where additional domain adaptation enhances overall effectiveness. Discussion We introduced a novel medical LLM family including, Me-LLaMA 13B and Me-LLaMA 70B, which encode comprehensive medical knowledge, along with their chat-optimized variants: Me-LLaMA-13/70B-chat, with strong zero-shot learning ability, for medical applications. These models were developed through the continual pre-training and instruction tuning of LLaMA2models,usingthe largestand mostcomprehensivebiomedicaland clinical data. Compared to existing studies, we perform the most compre- hensive evaluation, covering six critical text analysis tasks. Our evaluations reveal that Me-LLaMA models outperform existing open-source medical LLMs in various learning scenarios, showing less susceptibility to cata- strophic forgetting and achieving competitive results against major com- mercialmodelsincludingChatGPTandGPT-4.Ourworkpavesthewayfor more accurate,reliable,and comprehensive medical LLMs, and underscores the potential of LLMs on medical applications. Despitethesestrengths,weobservedcertainchallengesinspeciﬁctasks, such as NER and RE14,15, where even advanced models like GPT-4 exhibited low performance. When compared with other NLP tasks with higher per- formance, we noticed that one of the main reasons for low performance is that LLMs’ responses often lacked the conciseness and precision expected, with instances of missing outputs noted. The unexpected outputs also cause signiﬁcant challenges to automatic evaluation metrics. Therefore, more investigation is needed to further improve medical LLMs’ performance across tasks in the zero-shot setting and enhance the automatic assessment of these medical LLMs’ zero-shot capabilities. For the complex clinical case diagnosis, the Me-LLaMA-chat model had competitive performance and even outperformed GPT-4 in human evaluation. Existing studies have demonstrated GPT-4 is arguably one of the strongest LLMs in this task16. The robust performance of Me-LLaMA showed potential in assisting challenging clinical applications. It is noticed that variations in test sizesand evaluation methods across different studies contribute to the observed dif- ferences in performance betweenGPT-4 in our paper and other studies. We also noted that both the Me-LLaMA-chat model and GPT-4 faced difﬁ- culties identifying the correct diagnosis within the top ranks, underscoring the difﬁculty of this task. Additionally, while the NEJM CPCs offer a rig- orous test for these models, they do not encompass the full range of a physician’s duties or broader clinical competence. Therefore, complex clinical diagnosis remains a challenging area that demands more effective models and improved evaluation benchmarks to better capture the com- plexities of real-world clinical scenarios. Our results also emphasize the importance of data diversity during model development. Our empirical results revealed that the PMC-LLaMA 13B model, which employed a data mix ratio of 19:1 between medical and general domain data, exhibited around 2.7% performance drop across both general and biomedical tasks. On the other hand, the Meditron models, 7B, Model performance in the complex clinical case diagnosis task under automatic evaluation. The ﬁgure presents the top-K accuracy (where 1 K 5) of Me-LLaMA-70B-chat, ChatGPT, GPT-4, and LLaMA2-70B-chat on a complex clinical case diagnosis task, evaluated automatically. Model performance in the complex clinical case diagnosis task under human evaluation. The ﬁgure shows the top-1 and top-5 accuracy of Me-LLaMA/- 70B-chat and GPT-4 in a complex clinical case diagnosis task, evaluated through human assessment. https://doi.org/10.1038/s41746-025-01533-1 Article npj Digital Medicine | 8:141 4 and 70B, with a 99:1 mix ratio, demonstrated improvements in biomedical tasks, yet they still saw around 1% declines in the performance of general tasks. In contrast, our models, which adopt a 4:1 ratio, have shown enhancementsintheirperformanceforbothgeneralandmedicaltasks.This suggests that the integration of general domain data plays a vital role in mitigating the knowledge-forgetting issue during pre-training2,9. However, determining the optimal balance between general domain data and spe- cialized medical data is nontrivial, requiring careful empirical analysis. Future studies should examine methods to better determine the opti- mal ratio. The cost-effectiveness of instruction tuning is another important consideration. Pre-training, exempliﬁed by the LLaMA2 70B model, is notably resource-heavy, requiring about 160*700 GPU hours per epoch. Conversely, instruction tuning is far less resource-demanding, needing roughly 8*70 GPU hours per epoch, making it much more affordable than pre-training. While continual pre-training aims to incorporate specialized medical knowledge into the model, the observed performance improve- ments, particularly for smaller models like Me-LLaMA 13B, are relatively modest. This limited improvement can be attributed to several factors. First, theamountofdomain-speciﬁcpre-trainingdatausedissigniﬁcantlysmaller compared to the original pre-training data of LLaMA2, which exceeds 2 T tokens. This discrepancy suggests that larger amounts of domain-speciﬁc data may be required to fully activate the model’s potential. Second, the continual pre-training strategy itself could be optimized further. As noted earlier, the process faces challenges such as catastrophic forgetting, where themodellosesgeneral-domainknowledgeduringadaptationtospecialized data. Despite this, models trained only with the instruction tuning demonstrate that instruction tuning alone can signiﬁcantly enhance per- formance at a fraction of the computational cost. This highlights instruction tuning as a practical and cost-effective alternative, particularly in scenarios where computational resources are limited. The Me-LLaMA models, available in both 13B and 70B sizes, as well as in base and chat-optimized versions, enable a wide array of medical appli- cations, guided by the crucial balance between model size and resource availability. The base models provide a strong foundation for supervised ﬁne-tuning on specialized tasks, while the chat-optimized versions excel in instruction-following and zero-shot scenarios. Larger models, like the 70B, deliver superior reasoning capabilities but require signiﬁcant computational resources, making the 13B models a practical alternative for broader accessibility. Notably, the Me-LLaMA 13B model achieves performance comparable to its 70B counterpart across most datasets, demonstrating its utility for diverse medical tasks in resource-limited settings. These features suggest that Me-LLaMA models could be explored for various medical applications. Potential areas of use include: clinical decision support, where these models might assist in analyzing patient records, generating differential diagnoses, and synthesizing medical literature to support evidence-based decision-making; medical education, where chat- optimized versions could serve as interactive tools for teaching medical students and trainees by providing explanations for complex medical topics and simulating diagnostic reasoning; and administrative tasks, where these models may help streamline workﬂows by summarizing clinical notes and generating discharge summaries, potentially reducing the doc- umentation burden on clinicians. Further research and evaluation are warranted to assess Me-LLaMA’s real-world effectiveness and limitations in these clinical application settings. Despite these advancements, it is crucial to acknowledge the current Me-LLaMA models still have certain limitations that require further attention. Like all existing LLMs, they are susceptible to generating infor- mation with factual errors or biased information. To mitigate this, future studies could incorporate methodologies like reinforcement learning from humanfeedback(RLHF)17.Thisapproachcouldalignthemodels’responses more closely with human values and ensure they are grounded in factual medical knowledge. Another limitation is the current token handling capacity, capped at 4096 tokens, which is a constraint inherited from the backbone LLaMA2 model. Addressing this limitation could involve extending the models’ capability to handle longer contexts. This could be achieved by integrating advanced attention techniques, such as sparse local attention18, that are able to handle extensive contexts. Additionally, while MIMIC is the largest publicly available EHR dataset, its size is still relatively small compared to other data sources, which The comparison of zero-shot performances among Me-LLaMA models and their backbone models LLaMA2 Dataset Metric LLaMA2 13B (backbone) Me-LLaMA 13B (backbone + pre- train only) LLaMA2 13B- instruct (backbone + instruction tuning only) Me-LLaMA- 13B-chat (backbone + pre-train + instruction tuning) LLaMA2 70B (backbone) Me-LLaMA 70B (backbone + pre- train only) LLaMA2 70B- instruct (backbone + instruction tuning only) Me-LLaMA- 70B-chat (backbone + pre-train + instruction tuning) PubMedQA Acc 0.216 0.266 0.436 0.700 0.132 0.682 0.764 0.768 Macro-F1 0.177 0.250 0.416 0.504 0.152 0.520 0.531 0.557 MedQA Acc 0.000 0.000 0.013 0.427 0.005 0.281 0.499 0.523 Macro-F1 0.000 0.000 0.024 0.422 0.009 0.350 0.493 0.521 MedMCQA Acc 0.003 0.003 0.014 0.449 0.012 0.447 0.501 0.539 Macro-F1 0.006 0.005 0.029 0.440 0.024 0.396 0.493 0.538 EmrQA Acc 0.000 0.005 0.050 0.048 0.000 0.021 0.181 0.119 F1 0.038 0.122 0.286 0.307 0.000 0.172 0.399 0.346 i2b2 Macro-F1 0.008 0.030 0.232 0.263 0.181 0.224 0.245 0.329 DDI Macro-F1 0.035 0.036 0.164 0.214 0.034 0.118 0.121 0.283 HoC Macro-F1 0.253 0.210 0.194 0.335 0.255 0.252 0.563 0.544 MTsample Macro-F1 0.042 0.072 0.176 0.229 0.066 0.226 0.364 0.384 PubMed R-L 0.170 0.168 0.183 0.116 0.167 0.119 0.112 0.169 BERTS 0.654 0.654 0.667 0.445 0.654 0.654 0.601 0.678 MIMIC-CXR R-L 0.051 0.172 0.360 0.400 0.059 0.137 0.367 0.418 BERTS 0.566 0.697 0.791 0.797 0.577 0.649 0.784 0.787 BioNLI Macro-F1 0.109 0.060 0.185 0.195 0.285 0.499 0.345 0.436 MedNLI Macro-F1 0.172 0.206 0.457 0.472 0.265 0.256 0.657 0.675 https://doi.org/10.1038/s41746-025-01533-1 Article npj Digital Medicine | 8:141 5 may impact Me LLaMA’s generalizability on real-world scenarios. This limitation stems primarily from privacy concerns surrounding clinical data, which signiﬁcantly restrict the availability of large-scale EHR datasets. In this study, we included only MIMIC in the trainingof Me LLaMA because it is readily available for public dissemination through the established MIMIC data access procedures. Moving forward, we plan to train Me LLaMA on much larger proprietary clinical datasets, such as EHRs from Yale New Haven Health and the University of Florida Health. However, the terms of distribution and dissemination for models trained on such proprietary data will need to be carefully negotiated with our institutions’ data governance committees to ensure the safety and conﬁdentiality of clinical data. Methods We utilized LLaMA26 as the backbone model and developed Me-LLaMA through the process of continual pre-training and instruction tuning of LLaMA2, using 129B tokens and 214 K instruction tuning samples from general,biomedical,andclinicaldomains. study. presents the comparison of Me-LLaMA models and existing open source medical LLMs. Continual pre-training data To effectively adapt backbone LLaMA2 models for the medical domain through continual pre-training, we developed a mixed continual pre- training dataset, comprised of biomedical literature, clinical notes, and general domain data. Our dataset integrates a vast collection of biomedical literature from PubMed Central and PubMed Abstracts, sourced from the Pile dataset19. The PubMed Central subset includes 3,098,931 biomedical articles, and the PubMed Abstracts section encompasses abstracts from 15,518,009 documents. This comprehensive biomedical dataset provides a rich source of medical knowledge and research ﬁndings. To incorporate Overview of the study. Our study has three main components including pre-training, instruc- tion ﬁne-tuning and evaluation. Pre-training: we ﬁrst developed the Me-LLaMA base models by continual pre-training LLaMA2 with 129 billion tokens from mixed pre-training text data. Instruc- tion ﬁne-tuning: Me-LLaMA-chat models were further developed by instruction-tuning Me- LLaMA base models with 214 K instructions. Eva- luation: Finally, we evaluated the Me-LLaMA base models in a supervised learning setting across six text analysis tasks, and the Me-LLaMA-chat models in a zero-shot setting on both text analysis tasks and a clinical diagnosis task. The comparison of Me-LLaMA models and existing open source medical LLMs Model Backbone Model size Biomedical literature Clinical notes Continual pre- training (# of tokens) Instruction tuning (# of instructions) Evaluation tasks Release date MedAlpaca3 LLaMA 7/13B ✓ ✗ - 160 K QA 04/14/2023 ChatDoctor12 LLaMA2 7B ✓ ✗ - 100 K QA 05/24/2023 AlpaCare28 LLaMA 7/13B ✓ ✗ - 52 K QA, Summarization 10/23/2023 Clinical LLaMA11 LLaMA 7B ✗ ✓ - - Classiﬁcation 07/06/2023 Meditron10 LLaMA2 7/70B ✓ ✗ 48B - QA 11/27/2023 PMC-LLaMA2 LLaMA 7/13B ✓ ✗ 79B 514 K QA 04/27/2023 Me-LLaMA LLaMA2 13/70B ✓ ✓ 129B 214 K QA, NER, RE, Classiﬁcation, Summarization, NLI, Medical Diagnosis 06/05/2024 https://doi.org/10.1038/s41746-025-01533-1 Article npj Digital Medicine | 8:141 6 real-world clinical scenarios and reasoning, we included de-identiﬁed free- text clinical notes from MIMIC-III20, MIMIC-IV21, and MIMIC-CXR22. MIMIC-III contains 112,000 clinical reports records. MIMIC-IV contains 331,794 de-identiﬁed discharge summariesand 2,321,355radiologyreports. MIMIC-CXR adds further depth with 227,835 radiology reports for radiographic studies. Moreover, to prevent the model from forgetting acquired general knowledge, we incorporated a subset from the RedPajama23 dataset, a replication of LLaMA2’s pre-training data. This dataset is composed of diverse data slices, including processed Common- Crawl dumps, GitHub data, scientiﬁc articles from arXiv, a subset of Wikipedia pages, and popular websites from StackExchange. Our dataset was structured with a 15:1:4 ratio of biomedical, clinical, to general domain data and contains a total of 129 billion tokens, making it the largest pre- training dataset in the medical domain currently available. Medical instruction tuning data To enhance our model’s ability to follow instructions and generalize across diverse medical tasks, we further developed a novel medical instruction tuning dataset with 214,595 high-quality samples from a wide array of data sources. This dataset stands out from those used in existing medical LLMs due to its comprehensive coverage of both biomedical and clinical domains. Our data sources included biomedical literature, clinical notes, clinical guidelines, wikidoc, knowledge graphs, and general domain data, as shown in The diverse tasks aim to reﬁne the model’s ability to process and respond to medical information accurately and contextually. Detailed prompts for each data and the data example are shown in the Supple- mentary Information, Supplementary Training details As shown in we developed the Me-LLaMA 13B and 70B base models by continually pre-training the LLaMA2 13B and 70B models. These base models were then instruction-tuned to create the Me-LLaMA-13B-chat and Me-LLaMA-70B-chat models. The ﬁrst phase aims to develop Me-LLaMA base models, and adapt LLaMA2 models to better understand and generate text relevant to the medical context using the pre-training datasets we constructed. The objective is to enhance the model’s ability to understand and generate domain-speciﬁc text by optimizing it to predict the next word in a sequence based on the preceding context. This training was executed on the University of Florida’s HiPerGator AI supercomputer with 160 A100 80GB GPUs. We employed the AdamW optimizer with hyperparameters set to β1 to 0.9 and β2 to 0.95, alongside a weight decay of 0.00001 and a learning rate of 8e-6. We used a cosine learning rate scheduler with a 0.05 warmup ratio for gradual adaptation to training complexity and bf16 precision for com- putational efﬁciency. Gradient accumulation was set to 16 steps, and training was limited to one epoch. We utilized DeepSpeed24 for model parallelism. We further ﬁne-tuned Me-LLaMA base models to develop Me- LLaMA chat models, using the developed 214k instruction samples. In this phase, the models are trained to produce accurate and contextually appropriate responses to speciﬁc input instructions. Executed using 8 A100 GPUs, the ﬁne-tuning process was set to run for 3 epochs with a learning rate of 1e-5. We used a weight decay of 0.00001 and a warmup ratio of 0.01 for regularization and gradual learning rate increase. We utilized LoRA- based25 parameter-efﬁcient ﬁne-tuning. Evaluation benchmark Existing studies2,3,9 in the medical domain have primarily focused on eval- uating the QA task. In this study, we build an extensive medical evaluation benchmark (MIBE), encompassing six critical text analysis tasks: QA, NER, RE, Text Classiﬁcation, Text Summarization and NLI. These tasks collec- tivelyinvolve12datasetsmeticulouslysourcedfrombiomedical,andclinical domains as shown in We further assessed the effectiveness of Me-LLaMA in diagnosing complex clinical cases, a critical task given the increasing burden of diseases and the need for timely and accurate diagnosis to support clinicians. Recent studiesdemonstratethatLLMshavethepotentialtoaddressthischallenge26. Speciﬁcally, we evaluated the diagnostic accuracy of Me-LLaMA on 70 challenging medical cases from the New England Journal of Medicine clinicopathologic conferences (NEJM CPCs) published between January 2021 and December 2022, as collected from an existing study27. The NEJM CPCs are well-known for their unique and intricate clinical cases, which have long been used as benchmarks for evaluating challenging medical scenarios. In line with previous research26,27, we employed automatic eva- luations based on top-K (where k = 1,2,3,4,5) accuracy, deﬁned as the per- centage of cases where the correct diagnosis appeared within the top-K positions of the differential diagnosis list predicted by the assessed models. WeutilizedGPT-4o,astate-of-the-art(SOTA)LLM,toautomaticallyassess The overall instruction tuning dataset Task Type Source Size Copy right General Conversation Alpaca29 20,000 CC-BY-NC 4.0 Dolly30 CC-BY-SA-3.0 ShareGPT31 Apache-2.0 Biomedical Conversation HealthCareMagic10 20,000 Reserved by HealthCareMagic and Icliniq Icliniq10 Instructions MedInstruct11 52,000 CC BY-NC 4.0 Question Answering Medical Flash Cards3 34,000 No commercialized use MEDIQA32 2,220 CC BY 4.0 MedicationQA33 690 CC BY 4.0 LiveQA34 634 CC BY 4.0 WikiDocPatient3 5490 CC BY-SA 4.0 GuidelineQA 2000 Common Crawl (other) Summarization PubMed Central 10,000 CC BY Next Sentence Generation PubMed Central 20,000 CC BY Key words prediction PubMed Central 10,000 CC BY Causal Relation Detection PubMed35 2450 CC BY Relation Extraction UMLS knowledge graph2 10,000 Openrail Clinical QA, summarization, classiﬁcation, mortality prediction MIMIC-III20, MIMIC-IV21 30,000 PhysioNet credentialed health data use agreement 1.5.0 https://doi.org/10.1038/s41746-025-01533-1 Article npj Digital Medicine | 8:141 7 whether each diagnosis from the model’s differential diagnosis list matched the gold standard ﬁnal diagnosis, consistent with these prior studies. Existing studies27 have shown that LLM-based automatic calculation of top- K accuracy is comparable to human evaluation. Besides automatic evalua- tion, we had a clinician specializing in internal medicine perform a manual evaluation of top-k accuracy (k = 1, 5). For more details on data processing, automatic evaluation, and human evaluation, see the Supplementary Information. Evaluation settings We evaluated Me-LLaMA at two evaluation settings including zero-shot and supervised learning to evaluate their performance and generalization ability across various tasks compared to baseline models. Supervised learning In the supervised learning setting, we evaluated Me-LLaMA 13/70B base models’ performances adapted to downstream tasks. We conducted the task-speciﬁc ﬁnetuning on Me-LLaMA base models (Me-LLaMA task- speciﬁc) with each training set of assessed datasets in and then assessed the performance of Me-LLaMA task-speciﬁc models on test datasets. We employed the AdamW optimizer. For datasets with fewer than 10,000 training samples, we ﬁne-tuned the models for 5 epochs, while for larger datasets, the ﬁne-tuning was conducted for 3 epochs. A uniform learning rate of 1e-5 was used across all datasets. Our baseline models including LLaMA2 Models (7B/13B/70B)6: they are open-sourced LLMs released by Meta AI. PMC-LLaMA 13B2 is a biomedical LLM continually pre-trained on biomedical papers and medical books. Meditron7B/70B9: these are medical LLMs based on LLaMA2-7B/70B, continually pre-trained with a mix of clinical guidelines, medical papers and abstracts. Zero-shot Learning We assessed our Me-LLaMA 13/70B-chat models’ zero-shot learning cap- abilities, which are key for new task understanding and response without speciﬁc prior training. We compared our models and baseline models’ zero- shot, using standardized prompts (detailed in the Supplementary Infor- mation, Supplementary for each test dataset from We compared Me-LLaMA 13/70B-chat models with the following baseline models: ChatGPT/GPT-44: SOTA commercialized LLMs. We used the version of “gpt-3.5-turbo-0301” for ChatGPT, and the version of “gpt-4- 0314” for GPT-4. LLaMA2-7B/13B/70B-chat6 models were adaptations of the LLaMA2 series, optimized for dialogue and conversational scenarios. Medalpaca-7B/13B3 models were based on LLaMA-7B/13B, speciﬁcally ﬁne-tuned for tasks in the medical domain. The PMC-LLaMA-13B-chat2 model is an instruction-tuned medical LLM based on PMC-LLaMA-13B. The AlpaCare-13B11 model is speciﬁcally tailored for clinical tasks based on LLaMA-2 13B by instruction tuning. Meditron 70B9 is a medical LLM, continually pre-trained with a mix of clinical guidelines, biomedical papers, and abstracts based on LLaMA2 70B. Data availability All datasets employed in the continual pre-training process and evaluation are accessible from their original published venues. The PubMed Central and PubMed Abstracts subset from The Pile are available at https:// huggingface.co/datasets/EleutherAI/pile. MIMIC-IV and MIMIC-CXR datasets can be accessed under the PhysioNet Credentialed Health Data Use Agreement 1.5.0 at https://physionet.org/content/mimic-iv-note/2.2/ and https://physionet.org/content/mimic-cxr/2.0.0/ respectively. The Red- Pajama data is open-released at https://huggingface.co/datasets/ togethercomputer/RedPajama-Data-1. Alpaca data is openly released at: https://github.com/tatsu-lab/stanford_alpaca. Dolly data is openly released at: https://huggingface.co/datasets/databricks/databricks-dolly-15k. Share GPT data can be accessed at: https://huggingface.co/datasets/ anon8231489123/ShareGPT_Vicuna_unﬁltered. The clinical instruction tuning data based on MIMIC-IV and MIMIC-CXR can be accessed under the PhysioNet Credentialed Health Data Use Agreement 1.5.0 through: https://huggingface.co/clinicalnlplab. The Medical Flash Cards and wikidoc QA datasets can be accessed at https://huggingface.co/medalpaca. Other remaining instruction tuning data can be openly accessed at: https:// huggingface.co/clinicalnlplab. Me-LLaMA 13B and Me-LLaMA 70B models can be accessed at: https://physionet.org/content/me-llama/1.0.0/, subject to the completion of a credentialed health data use agreement. Code availability The code used for evaluation is available at: https://github.com/BIDS-Xu- Lab/ Me-LLaMA. Received: 14 November 2024; Accepted: 20 February 2025;